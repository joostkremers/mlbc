<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-11-22 Sun 20:22 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Deep Learning Bookcamp, ch. 2</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Joost Kremers" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/bigblow_theme/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/bigblow_theme/css/bigblow.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/bigblow_theme/css/hideshow.css"/>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery-ui-1.10.2.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery.localscroll-min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery.scrollTo-1.4.3.1-min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/jquery.zclip.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/bigblow.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/bigblow_theme/js/hideshow.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Deep Learning Bookcamp, ch. 2</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org2995ca5">1. Setup</a></li>
<li><a href="#org063ec79">2. Predicting car prices</a>
<ul>
<li><a href="#orgb41d646">2.1. Imports</a></li>
<li><a href="#org0a4c6f9">2.2. Reading and preparing the data</a></li>
<li><a href="#orge72acab">2.3. Setting up the validation framework</a>
<ul>
<li><a href="#orgd1bfae6">2.3.1. Splitting the data into a train, a validation and a test set</a></li>
<li><a href="#orgf35b36d">2.3.2. Training the model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org2995ca5" class="outline-2">
<h2 id="org2995ca5"><span class="section-number-2">1</span> Setup</h2>
<div class="outline-text-2" id="text-1">
<div class="org-src-container">
<pre class="src src-emacs-lisp">(pyvenv-workon "car-price-QhnioYpP-py3.8")
</pre>
</div>
</div>
</div>

<div id="outline-container-org063ec79" class="outline-2">
<h2 id="org063ec79"><span class="section-number-2">2</span> Predicting car prices</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-orgb41d646" class="outline-3">
<h3 id="orgb41d646"><span class="section-number-3">2.1</span> Imports</h3>
<div class="outline-text-3" id="text-2-1">
<div class="org-src-container">
<pre class="src src-python">import pandas as pd
import numpy as np

from matplotlib import pyplot as plt
import seaborn as sns
</pre>
</div>
</div>
</div>

<div id="outline-container-org0a4c6f9" class="outline-3">
<h3 id="org0a4c6f9"><span class="section-number-3">2.2</span> Reading and preparing the data</h3>
<div class="outline-text-3" id="text-2-2">
<div class="org-src-container">
<pre class="src src-python">df = pd.read_csv('data.csv')
len(df)
</pre>
</div>

<pre class="example">
11914
</pre>


<div class="org-src-container">
<pre class="src src-python">df.head()
</pre>
</div>

<pre class="example">
  Make       Model  Year             Engine Fuel Type  ...  highway MPG  city mpg Popularity   MSRP
0  BMW  1 Series M  2011  premium unleaded (required)  ...           26        19       3916  46135
1  BMW    1 Series  2011  premium unleaded (required)  ...           28        19       3916  40650
2  BMW    1 Series  2011  premium unleaded (required)  ...           28        20       3916  36350
3  BMW    1 Series  2011  premium unleaded (required)  ...           28        18       3916  29450
4  BMW    1 Series  2011  premium unleaded (required)  ...           28        18       3916  34500

[5 rows x 16 columns]
</pre>


<div class="org-src-container">
<pre class="src src-python">df.dtypes
</pre>
</div>

<pre class="example" id="org605f13f">
Make                  object
Model                 object
Year                   int64
Engine Fuel Type      object
Engine HP            float64
Engine Cylinders     float64
Transmission Type     object
Driven_Wheels         object
Number of Doors      float64
Market Category       object
Vehicle Size          object
Vehicle Style         object
highway MPG            int64
city mpg               int64
Popularity             int64
MSRP                   int64
dtype: object
</pre>

<p>
Cleaning up the data:
</p>

<div class="org-src-container">
<pre class="src src-python">df.columns = df.columns.str.lower().str.replace(' ', '_')

string_columns = list(df.dtypes[df.dtypes == 'object'].index)

for col in string_columns:
    df[col] = df[col].str.lower().str.replace(' ', '_')
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python">df.head()
</pre>
</div>

<pre class="example">
  make       model  year             engine_fuel_type  engine_hp  engine_cylinders transmission_type  ...                        market_category  vehicle_size vehicle_style highway_mpg city_mpg  popularity   msrp
0  bmw  1_series_m  2011  premium_unleaded_(required)      335.0               6.0            manual  ...  factory_tuner,luxury,high-performance       compact         coupe          26       19        3916  46135
1  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                     luxury,performance       compact   convertible          28       19        3916  40650
2  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                luxury,high-performance       compact         coupe          28       20        3916  36350
3  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                     luxury,performance       compact         coupe          28       18        3916  29450
4  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                                 luxury       compact   convertible          28       18        3916  34500

[5 rows x 16 columns]
</pre>


<ul class="org-ul">
<li>Notes:
<ul class="org-ul">
<li><code>df.dtypes</code> gives a list of types, <code>df.dtypes[df.dtypes =</code> 'object']= lists
only those that have the given type.</li>
<li><code>df.dtypes.index</code> gives an Index object listing all the relevant columns.</li>
<li>The <code>str</code> attribute makes it possible to apply string operations to all the
elements in the column at once.</li>
</ul></li>
</ul>

<div class="org-src-container">
<pre class="src src-python">sns.displot(df.msrp, kde=False)
plt.savefig('figures/figure2-1.png')
'figures/figure2-1.png'
</pre>
</div>


<div id="org9ba409f" class="figure">
<p><img src="figures/figure2-1.png" alt="figure2-1.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-python">sns.displot(df.msrp[df.msrp &lt; 100000], kde=False)
plt.savefig('figures/figure2-2.png')
'figures/figure2-2.png'
</pre>
</div>


<div id="org2ea3912" class="figure">
<p><img src="figures/figure2-2.png" alt="figure2-2.png" />
</p>
</div>

<p>
This kind of distribution is difficult for machine learning algorithms, esp.
linear regression, because of the long tail of high prices, which occur
relatively rarely, but must still be learned.
</p>

<p>
The common solution in such cases is to apply a logarithm transformation to the
<b>target value</b>:
</p>

<p>
y<sub>new</sub> = log(y+1)
</p>

<p>
Adding 1 to the original target value avoids calculating log(0) = -∞. Numpy has
a function for this purpose, <code>np.log1p</code>:
</p>

<div class="org-src-container">
<pre class="src src-python">log_price = np.log1p(df.msrp)
sns.displot(log_price)
plt.savefig('figures/figure2-3.png')
'figures/figure2-3.png'
</pre>
</div>


<div id="org36d7ae1" class="figure">
<p><img src="figures/figure2-3.png" alt="figure2-3.png" />
</p>
</div>

<p>
This so-called "normal or Gaussian distribution" is more amenable to machine
learning algorithms.
</p>

<div class="org-src-container">
<pre class="src src-python">df.isnull().sum()
</pre>
</div>

<pre class="example" id="orgedd9334">
make                    0
model                   0
year                    0
engine_fuel_type        3
engine_hp              69
engine_cylinders       30
transmission_type       0
driven_wheels           0
number_of_doors         6
market_category      3742
vehicle_size            0
vehicle_style           0
highway_mpg             0
city_mpg                0
popularity              0
msrp                    0
dtype: int64
</pre>
</div>
</div>

<div id="outline-container-orge72acab" class="outline-3">
<h3 id="orge72acab"><span class="section-number-3">2.3</span> Setting up the validation framework</h3>
<div class="outline-text-3" id="text-2-3">
</div>
<div id="outline-container-orgd1bfae6" class="outline-4">
<h4 id="orgd1bfae6"><span class="section-number-4">2.3.1</span> Splitting the data into a train, a validation and a test set</h4>
<div class="outline-text-4" id="text-2-3-1">
<ul class="org-ul">
<li>20% for validation</li>
<li>20% for testing</li>
<li>60% for training</li>
</ul>

<div class="org-src-container">
<pre class="src src-python">n = len(df)

n_val = int(0.2 * n)
n_test = int(0.2 * n)
n_train = n - (n_val + n_test)

np.random.seed(2)
idx = np.arange(n)
np.random.shuffle(idx)

df_shuffled = df.iloc[idx]

df_train = df_shuffled.iloc[:n_train].copy()
df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()
df_test = df_shuffled.iloc[n_train+n_val:].copy()
</pre>
</div>

<p>
We still need to apply the log transformation:
</p>

<div class="org-src-container">
<pre class="src src-python">y_train = np.log1p(df_train.msrp.values)
y_val = np.log1p(df_val.msrp.values)
y_test = np.log1p(df_test.msrp.values)
</pre>
</div>

<p>
The target value should be removed from the dataframes, just in case:
</p>

<div class="org-src-container">
<pre class="src src-python">del df_train['msrp']
del df_val['msrp']
del df_test['msrp']
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf35b36d" class="outline-4">
<h4 id="orgf35b36d"><span class="section-number-4">2.3.2</span> Training the model</h4>
<div class="outline-text-4" id="text-2-3-2">
</div>
<ol class="org-ol">
<li><a id="orgc5bfd1e"></a>Linear regression<br />
<div class="outline-text-5" id="text-2-3-2-1">
<p>
Computing the weights <code>w</code> can be done with the "normal equation":
</p>

<p>
w = (X<sup>T</sup>·X)<sup>-1</sup>·X<sup>T</sup>·y
</p>

<p>
where:
</p>

<ul class="org-ul">
<li>X is a matrix of input features</li>
<li>y is a vector of target values</li>
<li>X<sup>T</sup> is the <b>transpose</b> of X (<code>4X.T</code> in Numpy)</li>
<li>X<sup>-1</sup> is the <b>inverse</b> of X (<code>np.linalg.inv</code> in Numpy)</li>
</ul>

<p>
The dot product in Numpy is obtained with the <code>dot()</code> method. Thus, the formula
above becomes:
</p>

<p>
w = inv(X.T.dot(X)).dot(X.T).dot(y)
</p>
</div>
</li>

<li><a id="orgf634b44"></a>Implementing the normal equation<br />
<div class="outline-text-5" id="text-2-3-2-2">
<p>
In Python:
</p>

<div class="org-src-container">
<pre class="src src-python">def linear_regression(X, y):
    # X: matrix of features
    # y: vector of target values

    # Add a dummy column to accommodate the bias.
    ones = np.ones(X.shape[0])
    X = np.column_stack([ones, X])

    # Normal equation formula
    XTX = X.T.dot(X)
    XTX_inv = np.linalg.inv(XTX)
    w = XTX_inv.dot(X.T).dot(y)

    # Split the bias and the weights
    return w[0], w[1:]
</pre>
</div>
</div>
</li>

<li><a id="org9aec18a"></a>Predicting the price: baseline solution<br />
<div class="outline-text-5" id="text-2-3-2-3">
<p>
We select a few features to illustrate how things work:
</p>

<div class="org-src-container">
<pre class="src src-python">base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']
df_num = df_train[base]
df_num.head()
</pre>
</div>

<pre class="example">
       engine_hp  engine_cylinders  highway_mpg  city_mpg  popularity
2735       148.0               4.0           33        24        1385
6720       132.0               4.0           32        25        2031
5878       148.0               4.0           37        28         640
11190       90.0               4.0           18        16         873
4554       385.0               8.0           21        15        5657
</pre>


<p>
Replace any missing values with 0:
</p>

<div class="org-src-container">
<pre class="src src-python">df_num = df_num.fillna(0)
</pre>
</div>

<p>
This may not be the best way to deal with missing values, but it works.
</p>

<div class="remark" id="orgd009f68">
<p>
I guess what's not so great about it is that it reduces a term to zero in the
equation, causing the predicted price to be lower than one might expect. This is
the formula for predicting the price:
</p>

<p>
g(x) = w<sub>0</sub> + x<sub>1</sub>w<sub>1</sub> + x<sub>2</sub>w<sub>nil</sub><sub>2</sub> + x<sub>3</sub>w<sub>3</sub> + &#x2026;
</p>

<p>
Now if one feature is set to 0, the total sum g(x) is lower than it would have
been if the feature were not 0. A better solution might be to set unknown
features to the mean of that feature across all samples. That way the feature
still exerts its influence on the total price.
</p>

<p>
For example, if <code>city_mpg</code> is unknown, we may still assume that it isn't zero.
Setting it to zero would drive down the estimated price unreasonably. (Or drive
it up, depending on the relevant weight.)
</p>

</div>

<p>
Convert the data frame to a Numpy array. This is an important step, as the
data frame cannot be fed to the function <code>linear_regression</code>:
</p>

<div class="org-src-container">
<pre class="src src-python">X_train = df_num.values
</pre>
</div>

<p>
Now train the model:
</p>

<div class="org-src-container">
<pre class="src src-python">w_0, w = linear_regression(X_train, y_train)
</pre>
</div>

<div class="remark" id="orgc416769">
<p>
Note: Training the model means calculating the weights (well, duh!) Here, the
weights can simply be calculated, but formally it's still training.
</p>

</div>

<p>
Applying the model to the training data:
</p>

<div class="org-src-container">
<pre class="src src-python">y_pred = w_0 + X_train.dot(w)
</pre>
</div>

<p>
And plot the result:
</p>

<div class="org-src-container">
<pre class="src src-python">plt.clf()
sns.histplot(y_pred, label='pred')
sns.histplot(y_train, label='y')
plt.legend()
plt.savefig('figures/figure2-4.png')
'figures/figure2-4.png'
</pre>
</div>


<div id="org0fb11b3" class="figure">
<p><img src="figures/figure2-4.png" alt="figure2-4.png" />
</p>
</div>

<p>
Note: the predicted and the actual values are quite a bit apart. This is due to
the fact that the predictions here are based on only five features.
</p>

<div class="remark" id="orgf80c304">
<p>
The book uses <code>sns.distplot()</code>, which however gives a deprecation warning. One
should use <code>sns.displot()</code> or <code>sns.histplot()</code> instead, but only the latter
seems to allow overlaying two plots.
</p>

</div>
</div>
</li>

<li><a id="org28828fc"></a>Evaluating the model: Root Mean Square Error<br />
<div class="outline-text-5" id="text-2-3-2-4">
<p>
The Root Mean Square Error (RMSE) is a common measure for the quality of a
model:
</p>

<p>
\[
\mathrm{RMSE} = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(g(x_{i})-y_{i})^{2}}
\]
</p>

<p>
RMSE in Python using Numpy:
</p>

<div class="org-src-container">
<pre class="src src-python">def rmse(y, y_pred):
    # y: array of target values
    # y_pred: array of predicted values

    error = y_pred - y
    mse = (error ** 2).mean()
    return np.sqrt(mse)
</pre>
</div>

<p>
Note: Numpy does array operations. <code>y</code> and <code>y_pred</code> are arrays, which means that
<code>error</code> is, as well.
</p>

<p>
Computing the RMSE for the current model:
</p>

<div class="org-src-container">
<pre class="src src-python">rmse(y_train, y_pred)
</pre>
</div>

<pre class="example">
0.7554192603920132
</pre>


<p>
To compare the model with others, this measure should be computed on the
validation set, not the training set:
</p>

<div class="org-src-container">
<pre class="src src-python"># Create the matrix of validation samples X_val:
df_num = df_val[base]
df_num = df_num.fillna(0)
X_val = df_num.values

# Apply the model:
y_pred = w_0 + X_val.dot(w)

# Compute RMSE;
rmse(y_val, y_pred)
</pre>
</div>

<pre class="example">
0.7616530991301577
</pre>


<p>
To make this more easily repeatable:
</p>

<div class="org-src-container">
<pre class="src src-python">def prepare_X(df):
    df_num = df[base]
    df_num = df_num.fillna(0)
    X = df_num.values

    return X
</pre>
</div>

<p>
<code>prepare_X</code> creates a matrix from a data frame. Training and evaluation are now
simpler:
</p>

<div class="org-src-container">
<pre class="src src-python">X_train = prepare_X(df_train)
w_0, w = linear_regression(X_train, y_train)

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
print('validation:', rmse(y_val, y_pred))
</pre>
</div>

<pre class="example">
validation: 0.7616530991301577
</pre>
</div>
</li>

<li><a id="org0d59a00"></a>Simple feature engineering<br />
<div class="outline-text-5" id="text-2-3-2-5">
<p>
We can add new features based on the existing features. For example, the year a
car is produced is only a good predictor of price if it's interpreted as the age
of a car.
</p>

<p>
<code>df_train.year.max()</code> gives the newest car in the data set, which is 2017.
Subtract the year of a car from 2017 to get its age.
</p>

<div class="org-src-container">
<pre class="src src-python">def prepare_X(df):
    df = df.copy()
    features = base.copy()

    df['age'] = 2017 - df.year
    features.append('age')

    df_num = df[features]
    df_num = df_num.fillna(0)
    X = df_num.values

    return X
</pre>
</div>

<p>
Training and evaluation can now be done as follows:
</p>

<div class="org-src-container">
<pre class="src src-python">X_train = prepare_X(df_train)                    # Prepare the data.
w_0, w = linear_regression(X_train, y_train)     # Training the model.

X_val = prepare_X(df_val)                        # Apply the model to the validation set.
y_pred = w_0 + X_val.dot(w)
print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
</pre>
</div>

<pre class="example">
validation: 0.5172055461058327
</pre>


<p>
Plotting the distribution of the predicted values:
</p>

<div class="org-src-container">
<pre class="src src-python">plt.clf()
sns.histplot(y_pred, label='pred')
sns.histplot(y_val, label='y', color="red")
plt.legend()

plt.savefig('figures/figure2-5.png')
'figures/figure2-5.png'
</pre>
</div>


<div id="orga420c69" class="figure">
<p><img src="figures/figure2-5.png" alt="figure2-5.png" />
</p>
</div>
</div>
</li>

<li><a id="org058d7b7"></a>Categorical features<br />
<div class="outline-text-5" id="text-2-3-2-6">
<p>
Categorical features are features that take one of a limited set of values.
These are often strings, but may be numerical, as the number of doors of a car
(2, 3, or 4).
</p>

<p>
One way to handle categorical features in a model is to include a set of binary
features, one for each distinct value (called <b>one-hot encoding</b>). We can do
this in the <code>prepare_X</code> function:
</p>

<div class="org-src-container">
<pre class="src src-python">def prepare_X(df):
    """Prepare a data frame for ML.


    :param df: data frame of features

    :returns: feature matrix"""

    # Copy the data frame and the features.
    df = df.copy()
    features = base.copy()

    # Add some features
    df['age'] = 2017 - df.year
    features.append('age')

    for v in [2, 3, 4]:
        feature = 'num_doors_%s' % v
        value = (df['number_of_doors'] == v).astype(int)
        df[feature] = value
        features.append(feature)

    # Create a new data frame with only the features and add any missing features as 0.
    df_num = df[features]
    df_num = df_num.fillna(0)

    # Extract the values into a matrix and return the result.
    X = df_num.values
    return X
</pre>
</div>

<div class="remark" id="orgd8a4b83">
<p>
I'm not sure when a comparison can be turned into an integer&#x2026;
<code>True.astype(int)</code> returns an error, and so does <code>(1==0).astype(int)</code>, but for
some reason, <code>(df['number_of_doors'][0] =</code> 2).astype(int)= returns <code>1</code>.
</p>

<p>
Note that <code>(df['number_of_doors'] =</code> v)= is an array operation: it returns a
Pandas series of boolean values.
</p>

</div>

<p>
Doing the same for the feature make, taking only the five most frequently
occurring values:
</p>

<div class="org-src-container">
<pre class="src src-python">def prepare_X(df):
    """Prepare a data frame for ML.


    :param df: data frame of features

    :returns: feature matrix"""

    # Copy the data frame and the features.
    df = df.copy()
    features = base.copy()

    # Add some features
    df['age'] = 2017 - df.year
    features.append('age')

    for v in [2, 3, 4]:
        feature = 'num_doors_%s' % v
        value = (df['number_of_doors'] == v).astype(int)
        df[feature] = value
        features.append(feature)

    for v in ["chevrolet", "ford", "volkswagen", "toyota", "dodge"]:
        feature = 'is_make_%s' % v
        df[feature] = (df['make'] == v).astype(int)
        features.append(feature)

    # Create a new data frame with only the features and add any missing features as 0.
    df_num = df[features]
    df_num = df_num.fillna(0)

    # Extract the values into a matrix and return the result.
    X = df_num.values
    return X
</pre>
</div>

<p>
See if it works:
</p>

<div class="org-src-container">
<pre class="src src-python">X_train = prepare_X(df_train)                    # Prepare the data.
w_0, w = linear_regression(X_train, y_train)     # Training the model.

X_val = prepare_X(df_val)                        # Apply the model to the validation set.
y_pred = w_0 + X_val.dot(w)
print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
</pre>
</div>

<pre class="example">
validation: 0.5076038849556838
</pre>


<p>
Adding some more categorical features:
</p>

<div class="org-src-container">
<pre class="src src-python">def prepare_X(df):
    """Prepare a data frame for ML.


    :param df: data frame of features

    :returns: feature matrix"""

    # Copy the data frame and the features.
    df = df.copy()
    features = base.copy()

    # Add some features
    df['age'] = 2017 - df.year
    features.append('age')

    for v in [2, 3, 4]:
        feature = 'num_doors_%s' % v
        value = (df['number_of_doors'] == v).astype(int)
        df[feature] = value
        features.append(feature)

    for v in ["chevrolet", "ford", "volkswagen", "toyota", "dodge"]:
        feature = 'is_make_%s' % v
        df[feature] = (df['make'] == v).astype(int)
        features.append(feature)

    for v in ['regular_unleaded', 'premium_unleaded_(required)',
              'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:
        feature = 'is_type_%s' % v
        df[feature] = (df['engine_fuel_type'] == v).astype(int)
        features.append(feature)

    for v in ['automatic', 'manual', 'automated_manual']:
        feature = 'is_transmission_%s' % v
        df[feature] = (df['transmission_type'] == v).astype(int)
        features.append(feature)

    for v in ['front_wheel_drive', 'rear_wheel_drive',
              'all_wheel_drive', 'four_wheel_drive']:
        feature = 'is_driven_wheels_%s' % v
        df[feature] = (df['driven_wheels'] == v).astype(int)
        features.append(feature)

    for v in ['crossover', 'flex_fuel', 'luxury', 'luxury,performance', 'hatchback']:
        feature = 'is_mc_%s' % v
        df[feature] = (df['market_category'] == v).astype(int)
        features.append(feature)

    for v in ['compact', 'midsize', 'large']:
        feature = 'is_size_%s' % v
        df[feature] = (df['vehicle_size'] == v).astype(int)
        features.append(feature)

    for v in ['sedan', '4dr_suv', 'coupe', 'convertible', '4dr_hatchback']: 
        feature = 'is_style_%s' % v
        df[feature] = (df['vehicle_style'] == v).astype(int)
        features.append(feature)

    # Create a new data frame with only the features and add any missing features as 0.
    df_num = df[features]
    df_num = df_num.fillna(0)

    # Extract the values into a matrix and return the result.
    X = df_num.values
    return X
</pre>
</div>

<p>
Checking out the effect:
</p>

<div class="org-src-container">
<pre class="src src-python">X_train = prepare_X(df_train)                    # Prepare the data.
w_0, w = linear_regression(X_train, y_train)     # Train the model.

X_val = prepare_X(df_val)                        # Apply the model to the validation set.
y_pred = w_0 + X_val.dot(w)
print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
</pre>
</div>

<pre class="example">
validation: 22.322123465036622
</pre>


<p>
Adding these features makes the model much worse, not better.
</p>
</div>
</li>

<li><a id="org3d49a23"></a>Regularization<br />
<div class="outline-text-5" id="text-2-3-2-7">
<p>
The reason for the deterioration is <b>numerical instability</b>. The bias is very
large and so are some of the weights:
</p>

<div class="org-src-container">
<pre class="src src-python">print('bias: %s\nweights: %s' % (w_0, w))
</pre>
</div>

<pre class="example" id="orgce18a6d">
bias: 8991164041495205.0
weights: [-4.95981777e-02  6.73670308e+00  9.40777511e-01 -2.58497309e+00
  3.72822950e-03 -5.20036150e-01 -1.40699123e+03 -1.39430142e+03
 -1.39683545e+03 -5.60490940e+00 -2.27794179e+01  1.73041774e+01
 -4.30960052e+00 -9.23053458e+00  5.11353883e+01  5.56224498e+01
  4.86289752e+01  5.76621679e+01 -2.18304488e+02 -2.07996848e+02
 -2.72177915e+02 -8.99116404e+15 -8.99116404e+15 -8.99116404e+15
 -8.99116404e+15  6.13723252e+00  6.05470595e+00 -1.21844079e+00
  3.04348851e+00  1.20577703e+00 -2.16182997e+01 -2.60265778e+01
 -2.38977036e+01 -7.67460186e-02  4.14645821e-02  1.86187511e-01
  3.55798979e-01 -2.14066472e-01]
</pre>

<p>
The underlying cause of the problem is that the feature matrix becomes
<b>singular</b> or <b>undetermined</b>. This can happen when two features are essentially
the same, e.g., if there's a feature "miles per gallon" and you then add a
feature "kilometers per liter".
</p>

<p>
Technically, the matrix produced here is not singular, but the large bias and
weights indicate it's close.
</p>

<p>
This numerical instability can be solved using <b>regularization</b> techniques. One
way to do regularization is to add a small number to each diagonal element of
the matrix. The formula for linear regression then becomes:
</p>

<p>
w = (X<sup>T</sup>·X+αI)<sup>-1</sup>·X<sup>T</sup>·y
</p>

<p>
I is an identity matrix, α a constant. In Numpy:
</p>

<pre class="example" id="org9e84dfc">
XTX = X_train.T.dot(X_train)
XTX = XTX + 0.01 * np.eye(XTX.shape[0])
</pre>

<p>
Here, &alpha; is set to <code>0.01</code>. The function <code>np.eye()</code> creates a 2D identity matrix:
</p>

<div class="org-src-container">
<pre class="src src-python">0.01 * np.eye(4)
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">0.01</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0.01</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0.01</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0.01</td>
</tr>
</tbody>
</table>

<p>
Linear regression with regularization:
</p>

<div class="org-src-container">
<pre class="src src-python">def linear_regression_reg(X, y, r=0.01):
    ones = np.ones(X.shape[0])
    X = np.column_stack([ones, X])

    XTX = X.T.dot(X)
    reg = r * np.eye(XTX.shape[0])
    XTX = XTX + reg

    XTX_inv = np.linalg.inv(XTX)
    w = XTX_inv.dot(X.T).dot(y)

    return w[0], w[1:]
</pre>
</div>

<p>
A grid search suggests that values around 0.01 are fine. Smaller values do
reduce the RMSE, but only marginally.
</p>

<div class="org-src-container">
<pre class="src src-python">X_train = prepare_X(df_train)
w_0, w = linear_regression_reg(X_train, y_train, r=0.01)

X_val = prepare_X(df_val)
y_pred = w_0 + X_val.dot(w)
print('validation:', rmse(y_val, y_pred))

X_test = prepare_X(df_test)
y_pred = w_0 + X_test.dot(w)
print('test:', rmse(y_test, y_pred))
</pre>
</div>

<pre class="example">
validation: 0.46023949630840544
test: 0.45718136795913034
</pre>


<p>
The results suggest that the model works well.
</p>

<div class="remark" id="org076abaf">
<p>
Still, I'm not clear on whether the value of ≈0.46 is good or not. Does it mean
the model predicts the price well or not?
</p>

</div>
</div>
</li>

<li><a id="orgd0687ee"></a>Using the model<br />
<div class="outline-text-5" id="text-2-3-2-8">
<p>
When using the model to make a prediction, one needs to create a data frame with
one row. Take the following ad for a car:
</p>

<div class="org-src-container">
<pre class="src src-python">ad = {
    'city_mpg'          : 18,
    'driven_wheels'     : 'all_wheel_drive',
    'engine_cylinders'  : 6.0,
    'engine_fuel_type'  : 'regular_unleaded',
    'engine_hp'         : 268.0,
    'highway_mpg'       : 25,
    'make'              : 'toyota',
    'market_category'   : 'crossover,performance',
    'model'             : 'venza',
    'number_of_doors'   : 4.0,
    'popularity'        : 2031,
    'transmission_type' : 'automatic',
    'vehicle_size'      : 'midsize',
    'vehicle_style'     : 'wagon',
    'year'              : 2013
}
</pre>
</div>

<p>
Converting this to a data frame and a matrix:
</p>

<div class="org-src-container">
<pre class="src src-python">df_ad = pd.DataFrame([ad])
X_test = prepare_X(df_ad)
</pre>
</div>

<p>
Applying the model yields a value that is the logarithm of the predicted price.
To calculate the price, apply the exponent function:
</p>

<div class="org-src-container">
<pre class="src src-python">y_pred = w_0 + X_test.dot(w)
suggestion = np.expm1(y_pred)
print('suggested price: $%d' % round(suggestion[0]))
</pre>
</div>

<pre class="example">
suggested price: $28294
</pre>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Joost Kremers</p>
<p class="date">Created: 2020-11-22 Sun 20:22</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
