#+TITLE: Notes on "Machine Learning Bookcamp"
#+PROPERTY: header-args:python :exports both :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="styles/readtheorg_theme/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="styles/readtheorg_theme/css/readtheorg.css"/>
#+HTML_HEAD: <script type="text/javascript" src="styles/lib/js/jquery.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="styles/lib/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="styles/lib/js/jquery.stickytableheaders.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="styles/readtheorg_theme/js/readtheorg.js"></script>

* Introduction

* Setup                                                            :noexport:

Run the code in a virtual environment:

#+begin_src emacs-lisp :results silent
  (pyvenv-workon "mlbc-JThvy3A1-py3.8")
#+end_src

* Chapter 2: Machine Learning for Regression

** Notes to chapter 2                                             :noexport:
:PROPERTIES:
:header-args:python+: :session ch2-notes
:END:

*** Imports

#+begin_src python :results silent
  import pandas as pd
  import numpy as np

  from matplotlib import pyplot as plt
  import seaborn as sns
#+end_src

*** Reading and preparing the data

#+begin_src python :results value
  df = pd.read_csv('../data/cars.csv')
  len(df)
#+end_src

#+RESULTS:
: 11914

#+begin_src python
  df.head()
#+end_src

#+RESULTS:
:   Make       Model  Year             Engine Fuel Type  ...  highway MPG  city mpg Popularity   MSRP
: 0  BMW  1 Series M  2011  premium unleaded (required)  ...           26        19       3916  46135
: 1  BMW    1 Series  2011  premium unleaded (required)  ...           28        19       3916  40650
: 2  BMW    1 Series  2011  premium unleaded (required)  ...           28        20       3916  36350
: 3  BMW    1 Series  2011  premium unleaded (required)  ...           28        18       3916  29450
: 4  BMW    1 Series  2011  premium unleaded (required)  ...           28        18       3916  34500
:
: [5 rows x 16 columns]

#+begin_src python
  df.dtypes
#+end_src

#+RESULTS:
#+begin_example
Make                  object
Model                 object
Year                   int64
Engine Fuel Type      object
Engine HP            float64
Engine Cylinders     float64
Transmission Type     object
Driven_Wheels         object
Number of Doors      float64
Market Category       object
Vehicle Size          object
Vehicle Style         object
highway MPG            int64
city mpg               int64
Popularity             int64
MSRP                   int64
dtype: object
#+end_example

Cleaning up the data:

#+begin_src python :results silent
  df.columns = df.columns.str.lower().str.replace(' ', '_')

  string_columns = list(df.dtypes[df.dtypes == 'object'].index)

  for col in string_columns:
      df[col] = df[col].str.lower().str.replace(' ', '_')
#+end_src

#+begin_src python
  df.head()
#+end_src

#+RESULTS:
:   make       model  year             engine_fuel_type  engine_hp  engine_cylinders transmission_type  ...                        market_category  vehicle_size vehicle_style highway_mpg city_mpg  popularity   msrp
: 0  bmw  1_series_m  2011  premium_unleaded_(required)      335.0               6.0            manual  ...  factory_tuner,luxury,high-performance       compact         coupe          26       19        3916  46135
: 1  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                     luxury,performance       compact   convertible          28       19        3916  40650
: 2  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                luxury,high-performance       compact         coupe          28       20        3916  36350
: 3  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                     luxury,performance       compact         coupe          28       18        3916  29450
: 4  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                                 luxury       compact   convertible          28       18        3916  34500
:
: [5 rows x 16 columns]

- Notes:
  - =df.dtypes= gives a list of types, =df.dtypes[df.dtypes == 'object']= lists
    only those that have the given type.
  - =df.dtypes.index= gives an Index object listing all the relevant columns.
  - The =str= attribute makes it possible to apply string operations to all the
    elements in the column at once.

#+begin_src python :results file figures/figure2-01.png
  sns.displot(df.msrp, kde=False)
  plt.savefig('figures/figure2-01.png')
  'figures/figure2-01.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-01.png]]

#+begin_src python :results file figures/figure2-02.png
  sns.displot(df.msrp[df.msrp < 100000], kde=False)
  plt.savefig('figures/figure2-02.png')
  'figures/figure2-02.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-02.png]]

This kind of distribution is difficult for machine learning algorithms, esp.
linear regression, because of the long tail of high prices, which occur
relatively rarely, but must still be learned.

The common solution in such cases is to apply a logarithm transformation to the
*target value*:

y_{new} = log(y+1)

Adding 1 to the original target value avoids calculating log(0) = -∞. Numpy has
a function for this purpose, =np.log1p=:

#+begin_src python :results file figures/figure2-03.png
  log_price = np.log1p(df.msrp)
  sns.displot(log_price)
  plt.savefig('figures/figure2-03.png')
  'figures/figure2-03.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-03.png]]

This so-called "normal or Gaussian distribution" is more amenable to machine
learning algorithms.

Note that there are a lot of empty cells in the dataframe. These need to be
dealt with (see below):

#+begin_src python
  df.isnull().sum()
#+end_src

#+RESULTS:
#+begin_example
make                    0
model                   0
year                    0
engine_fuel_type        3
engine_hp              69
engine_cylinders       30
transmission_type       0
driven_wheels           0
number_of_doors         6
market_category      3742
vehicle_size            0
vehicle_style           0
highway_mpg             0
city_mpg                0
popularity              0
msrp                    0
dtype: int64
#+end_example

*** Setting up the validation framework

*** Splitting the data into a train, a validation and a test set
- 20% for validation
- 20% for testing
- 60% for training

#+begin_src python :results silent
  n = len(df)

  n_val = int(0.2 * n)
  n_test = int(0.2 * n)
  n_train = n - (n_val + n_test)

  np.random.seed(2)
  idx = np.arange(n)
  np.random.shuffle(idx)

  df_shuffled = df.iloc[idx]

  df_train = df_shuffled.iloc[:n_train].copy()
  df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()
  df_test = df_shuffled.iloc[n_train+n_val:].copy()
#+end_src

We still need to apply the log transformation:

#+begin_src python :results silent
  y_train = np.log1p(df_train.msrp.values)
  y_val = np.log1p(df_val.msrp.values)
  y_test = np.log1p(df_test.msrp.values)
#+end_src

The target value should be removed from the dataframes, just in case:

#+begin_src python :results silent
  del df_train['msrp']
  del df_val['msrp']
  del df_test['msrp']
#+end_src

*** Training the model

**** Linear regression

Computing the weights =w= can be done with the "normal equation":

w = (X^{T}·X)^{-1}·X^{T}·y

where:

- X is a matrix of input features
- y is a vector of target values
- X^{T} is the *transpose* of X (=X.T= in Numpy)
- X^{-1} is the *inverse* of X (=np.linalg.inv= in Numpy)


The dot product in Numpy is obtained with the =dot()= method. Thus, the formula
above becomes:

#+begin_src python
w = inv(X.T.dot(X)).dot(X.T).dot(y)
#+end_src

**** Implementing the normal equation

In Python:

#+begin_src python :results silent
  def linear_regression(X, y):
      # X: matrix of features
      # y: vector of target values

      # Add a dummy column to accommodate the bias.
      ones = np.ones(X.shape[0])
      X = np.column_stack([ones, X])

      # Normal equation formula
      XTX = X.T.dot(X)
      XTX_inv = np.linalg.inv(XTX)
      w = XTX_inv.dot(X.T).dot(y)

      # Split the bias and the weights
      return w[0], w[1:]
#+end_src

*** Predicting the price: baseline solution

We select a few features to illustrate how things work:

#+begin_src python
  base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']
  df_num = df_train[base]
  df_num.head()
#+end_src

#+RESULTS:
:        engine_hp  engine_cylinders  highway_mpg  city_mpg  popularity
: 2735       148.0               4.0           33        24        1385
: 6720       132.0               4.0           32        25        2031
: 5878       148.0               4.0           37        28         640
: 11190       90.0               4.0           18        16         873
: 4554       385.0               8.0           21        15        5657

Replace any missing values with 0:

#+begin_src python :results silent
  df_num = df_num.fillna(0)
#+end_src

This may not be the best way to deal with missing values, but it works.

#+begin_remark
I guess what's not so great about it is that it reduces a term to zero in the
equation, causing the predicted price to be lower than one might expect. This is
the formula for predicting the price:

g(x) = w_{0} + x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} + ...

Now if one feature is set to 0, the total sum g(x) is lower than it would have
been if the feature were not 0. A better solution might be to set unknown
features to the mean of that feature across all samples. That way the feature
still exerts its influence on the total price.

For example, if =city_mpg= is unknown, we may still assume that it isn't zero.
Setting it to zero would drive down the estimated price unreasonably. (Or drive
it up, depending on the relevant weight.)
#+end_remark

Convert the data frame to a Numpy array. This is an important step, as the
data frame cannot be fed to the function =linear_regression=:

#+begin_src python :results silent
  X_train = df_num.values
#+end_src

Now train the model:

#+begin_src python :results silent
  w_0, w = linear_regression(X_train, y_train)
#+end_src

#+begin_remark
Note: Training the model means calculating the weights (well, duh!) Here, the
weights can simply be calculated, but formally it's still training.
#+end_remark

Applying the model to the training data:

#+begin_src python :results silent
  y_pred = w_0 + X_train.dot(w)
#+end_src

And plot the result:

#+begin_src python :results file figures/figure2-04.png
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-04.png')
  'figures/figure2-04.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-04.png]]

Note: the predicted and the actual values are quite a bit apart. This is due to
the fact that the predictions here are based on only five features.

#+begin_remark
The book uses =sns.distplot()=, which however gives a deprecation warning. One
should use =sns.displot()= or =sns.histplot()= instead, but only the latter
seems to allow overlaying two plots.
#+end_remark

*** Evaluating the model: Root Mean Square Error

The Root Mean Square Error (RMSE) is a common measure for the quality of a
model:

\[
\mathrm{RMSE} = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(g(x_{i})-y_{i})^{2}}
\]

RMSE in Python using Numpy:

#+begin_src python :results silent
  def rmse(y, y_pred):
      # y: array of target values
      # y_pred: array of predicted values

      error = y_pred - y
      mse = (error ** 2).mean()
      return np.sqrt(mse)
#+end_src

Note: Numpy does array operations. =y= and =y_pred= are arrays, which means that
=error= is, as well.

Computing the RMSE for the current model:

#+begin_src python
  rmse(y_train, y_pred)
#+end_src

#+RESULTS:
: 0.7554192603920132

To compare the model with others, this measure should be computed on the
validation set, not the training set:

#+begin_src python
  # Create the matrix of validation samples X_val:
  df_num = df_val[base]
  df_num = df_num.fillna(0)
  X_val = df_num.values

  # Apply the model:
  y_pred = w_0 + X_val.dot(w)

  # Compute RMSE;
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 0.7616530991301577

To make this more easily repeatable:

#+begin_src python :results silent
  def prepare_X(df):
      df_num = df[base]
      df_num = df_num.fillna(0)
      X = df_num.values

      return X
#+end_src

=prepare_X= creates a matrix from a data frame. Training and evaluation are now
simpler:

#+begin_src python :results output
  X_train = prepare_X(df_train)
  w_0, w = linear_regression(X_train, y_train)

  X_val = prepare_X(df_val)
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))
#+end_src

#+RESULTS:
: validation: 0.7616530991301577

*** Simple feature engineering

We can add new features based on the existing features. For example, the year a
car is produced is only a good predictor of price if it's interpreted as the age
of a car.

=df_train.year.max()= gives the newest car in the data set, which is 2017.
Subtract the year of a car from 2017 to get its age.

#+begin_src python :results silent
  def prepare_X(df):
      df = df.copy()
      features = base.copy()

      df['age'] = 2017 - df.year
      features.append('age')

      df_num = df[features]
      df_num = df_num.fillna(0)
      X = df_num.values

      return X
#+end_src

Training and evaluation can now be done as follows:

#+begin_src python :results output
  X_train = prepare_X(df_train)                    # Prepare the data.
  w_0, w = linear_regression(X_train, y_train)     # Training the model.

  X_val = prepare_X(df_val)                        # Apply the model to the validation set.
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
#+end_src

#+RESULTS:
: validation: 0.5172055461058327

Plotting the distribution of the predicted values:

#+begin_src python :results file figures/figure2-05.png
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_val, label='y', color="red")
  plt.legend()

  plt.savefig('figures/figure2-05.png')
  'figures/figure2-05.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-05.png]]

*** Categorical features

Categorical features are features that take one of a limited set of values.
These are often strings, but may be numerical, as the number of doors of a car
(2, 3, or 4).

One way to handle categorical features in a model is to include a set of binary
features, one for each distinct value (called *one-hot encoding*). We can do
this in the =prepare_X= function:

#+begin_src python :results silent
  def prepare_X(df):
      # Copy the data frame and the features.
      df = df.copy()
      features = base.copy()

      # Add some features
      df['age'] = 2017 - df.year
      features.append('age')

      for v in [2, 3, 4]:
          feature = 'num_doors_%s' % v
          value = (df['number_of_doors'] == v).astype(int)
          df[feature] = value
          features.append(feature)

      # Create a new data frame with only the features and add any missing features as 0.
      df_num = df[features]
      df_num = df_num.fillna(0)

      # Extract the values into a matrix and return the result.
      X = df_num.values
      return X
#+end_src

#+begin_remark
I'm not sure when a comparison can be turned into an integer...
=True.astype(int)= returns an error, and so does =(1==0).astype(int)=, but for
some reason, =(df['number_of_doors'][0]==2).astype(int)= returns =1=.

Note that =(df['number_of_doors']==v)= is an array operation: it returns a
Pandas series of boolean values.
#+end_remark

Doing the same for the feature make, taking only the five most frequently
occurring values:

#+begin_src python :results silent
  def prepare_X(df):
      # Copy the data frame and the features.
      df = df.copy()
      features = base.copy()

      # Add some features
      df['age'] = 2017 - df.year
      features.append('age')

      for v in [2, 3, 4]:
          feature = 'num_doors_%s' % v
          value = (df['number_of_doors'] == v).astype(int)
          df[feature] = value
          features.append(feature)

      for v in ["chevrolet", "ford", "volkswagen", "toyota", "dodge"]:
          feature = 'is_make_%s' % v
          df[feature] = (df['make'] == v).astype(int)
          features.append(feature)

      # Create a new data frame with only the features and add any missing features as 0.
      df_num = df[features]
      df_num = df_num.fillna(0)

      # Extract the values into a matrix and return the result.
      X = df_num.values
      return X
#+end_src

See if it works:

#+begin_src python :results output
  X_train = prepare_X(df_train)                    # Prepare the data.
  w_0, w = linear_regression(X_train, y_train)     # Training the model.

  X_val = prepare_X(df_val)                        # Apply the model to the validation set.
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
#+end_src

#+RESULTS:
: validation: 0.5076038849556838

Adding some more categorical features:

#+begin_src python :results silent
  def prepare_X(df):
      # Copy the data frame and the features.
      df = df.copy()
      features = base.copy()

      # Add some features
      df['age'] = 2017 - df.year
      features.append('age')

      for v in [2, 3, 4]:
          feature = 'num_doors_%s' % v
          value = (df['number_of_doors'] == v).astype(int)
          df[feature] = value
          features.append(feature)

      for v in ["chevrolet", "ford", "volkswagen", "toyota", "dodge"]:
          feature = 'is_make_%s' % v
          df[feature] = (df['make'] == v).astype(int)
          features.append(feature)

      for v in ['regular_unleaded', 'premium_unleaded_(required)',
                'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:
          feature = 'is_type_%s' % v
          df[feature] = (df['engine_fuel_type'] == v).astype(int)
          features.append(feature)

      for v in ['automatic', 'manual', 'automated_manual']:
          feature = 'is_transmission_%s' % v
          df[feature] = (df['transmission_type'] == v).astype(int)
          features.append(feature)

      for v in ['front_wheel_drive', 'rear_wheel_drive',
                'all_wheel_drive', 'four_wheel_drive']:
          feature = 'is_driven_wheels_%s' % v
          df[feature] = (df['driven_wheels'] == v).astype(int)
          features.append(feature)

      for v in ['crossover', 'flex_fuel', 'luxury', 'luxury,performance', 'hatchback']:
          feature = 'is_mc_%s' % v
          df[feature] = (df['market_category'] == v).astype(int)
          features.append(feature)

      for v in ['compact', 'midsize', 'large']:
          feature = 'is_size_%s' % v
          df[feature] = (df['vehicle_size'] == v).astype(int)
          features.append(feature)

      for v in ['sedan', '4dr_suv', 'coupe', 'convertible', '4dr_hatchback']:
          feature = 'is_style_%s' % v
          df[feature] = (df['vehicle_style'] == v).astype(int)
          features.append(feature)

      # Create a new data frame with only the features and add any missing features as 0.
      df_num = df[features]
      df_num = df_num.fillna(0)

      # Extract the values into a matrix and return the result.
      X = df_num.values
      return X
#+end_src

Checking out the effect:

#+begin_src python :results output
  X_train = prepare_X(df_train)                    # Prepare the data.
  w_0, w = linear_regression(X_train, y_train)     # Train the model.

  X_val = prepare_X(df_val)                        # Apply the model to the validation set.
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
#+end_src

#+RESULTS:
: validation: 22.322123465036622

Adding these features makes the model much worse, not better.

*** Regularization

The reason for the deterioration is *numerical instability*. The bias is very
large and so are some of the weights:

#+begin_src python :results output
  print('bias: %s\nweights: %s' % (w_0, w))
#+end_src

#+RESULTS:
#+begin_example
bias: 8991164041495205.0
weights: [-4.95981777e-02  6.73670308e+00  9.40777511e-01 -2.58497309e+00
  3.72822950e-03 -5.20036150e-01 -1.40699123e+03 -1.39430142e+03
 -1.39683545e+03 -5.60490940e+00 -2.27794179e+01  1.73041774e+01
 -4.30960052e+00 -9.23053458e+00  5.11353883e+01  5.56224498e+01
  4.86289752e+01  5.76621679e+01 -2.18304488e+02 -2.07996848e+02
 -2.72177915e+02 -8.99116404e+15 -8.99116404e+15 -8.99116404e+15
 -8.99116404e+15  6.13723252e+00  6.05470595e+00 -1.21844079e+00
  3.04348851e+00  1.20577703e+00 -2.16182997e+01 -2.60265778e+01
 -2.38977036e+01 -7.67460186e-02  4.14645821e-02  1.86187511e-01
  3.55798979e-01 -2.14066472e-01]
#+end_example

The underlying cause of the problem is that the feature matrix becomes
*singular* or *undetermined*. This can happen when two features are essentially
the same, e.g., if there's a feature "miles per gallon" and you then add a
feature "kilometers per liter".

Technically, the matrix produced here is not singular, but the large bias and
weights indicate it's close.

This numerical instability can be solved using *regularization* techniques. One
way to do regularization is to add a small number to each diagonal element of
the matrix. The formula for linear regression then becomes:

w = (X^{T}·X+αI)^{-1}·X^{T}·y

I is an identity matrix, α a constant. In Numpy:

#+begin_example
  XTX = X_train.T.dot(X_train)
  XTX = XTX + 0.01 * np.eye(XTX.shape[0])
#+end_example

Here, \alpha is set to =0.01=. The function =np.eye()= creates a 2D identity matrix:

#+begin_src python
  0.01 * np.eye(4)
#+end_src

#+RESULTS:
| 0.01 |    0 |    0 |    0 |
|    0 | 0.01 |    0 |    0 |
|    0 |    0 | 0.01 |    0 |
|    0 |    0 |    0 | 0.01 |

Linear regression with regularization:

#+begin_src python :results silent
  def linear_regression_reg(X, y, r=0.01):
      ones = np.ones(X.shape[0])
      X = np.column_stack([ones, X])

      XTX = X.T.dot(X)
      reg = r * np.eye(XTX.shape[0])
      XTX = XTX + reg

      XTX_inv = np.linalg.inv(XTX)
      w = XTX_inv.dot(X.T).dot(y)

      return w[0], w[1:]
#+end_src

A grid search suggests that values around 0.01 are fine. Smaller values do
reduce the RMSE, but only marginally.

#+begin_src python :results output
  X_train = prepare_X(df_train)
  w_0, w = linear_regression_reg(X_train, y_train, r=0.01)

  X_val = prepare_X(df_val)
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))

  X_test = prepare_X(df_test)
  y_pred = w_0 + X_test.dot(w)
  print('test:', rmse(y_test, y_pred))
#+end_src

#+RESULTS:
: validation: 0.46023949630840544
: test: 0.45718136795913034

The results suggest that the model works well.

#+begin_remark
Still, I'm not clear on whether the value of approx. 0.46 is good or not. Does
it mean the model predicts the price well or not?
#+end_remark

*** Using the model

When using the model to make a prediction, one needs to create a data frame with
one row. Take the following ad for a car:

#+begin_src python :results silent
  ad = {
      'city_mpg'          : 18,
      'driven_wheels'     : 'all_wheel_drive',
      'engine_cylinders'  : 6.0,
      'engine_fuel_type'  : 'regular_unleaded',
      'engine_hp'         : 268.0,
      'highway_mpg'       : 25,
      'make'              : 'toyota',
      'market_category'   : 'crossover,performance',
      'model'             : 'venza',
      'number_of_doors'   : 4.0,
      'popularity'        : 2031,
      'transmission_type' : 'automatic',
      'vehicle_size'      : 'midsize',
      'vehicle_style'     : 'wagon',
      'year'              : 2013
  }
#+end_src

Converting this to a data frame and a matrix:

#+begin_src python :results silent
  df_ad = pd.DataFrame([ad])
  X_test = prepare_X(df_ad)
#+end_src

Applying the model yields a value that is the logarithm of the predicted price.
To calculate the price, apply the exponent function:

#+begin_src python :results output
  y_pred = w_0 + X_test.dot(w)
  suggestion = np.expm1(y_pred)
  print('suggested price: $%d' % round(suggestion[0]))
#+end_src

#+RESULTS:
: suggested price: $28294


** Exercises and code
:PROPERTIES:
:header-args:python+: :session ch2-ex
:END:

*** Utility functions
:PROPERTIES:
:header-args:python+: :tangle ../mlbc/general/utils.py :results silent
:END:

Convert the code from chapter 2 to a set of utility functions:

#+begin_src python
  import numbers

  import pandas as pd
  import numpy as np

#+end_src

Read the data and clean up the column names / alphanumeric data:

#+begin_src python
  def read_data(file):
      """Read a dataframe from a CSV file.

      Parameters:
      file (string): path to a CSV file.

      Returns:
      DataFrame holding the contents of the file.
      """
      df = pd.read_csv(file)
      return df


  def clean_alphanum_data(df):
      """Clean up alphanumeric data in a dataframe.

      Convert all strings to lower case and replace spaces with underscores.

      Parameters:
      df (DataFrame): the dataframe to be cleaned.

      Returns:
      None.
      """
      df.columns = df.columns.str.lower().str.replace(' ', '_')

      string_columns = list(df.dtypes[df.dtypes == 'object'].index)
      for col in string_columns:
          df[col] = df[col].str.lower().str.replace(' ', '_')

#+end_src

Split the data frame into a train, validation and test set:

#+begin_src python
  def split_data_frame(df, split=0.2, seed=None, test=True):
      """Split a dataframe into a train, validation and test set.

      The dataframe is first randomized and then split into three parts. If
      `validation` is False, the dataframe is split into two parts.

      Parameters:
      df (DataFrame): the dataframe to split.
      split (float):  fraction of the dataframe to use for validation and test sets.
      seed (int):     the seed used for randomization.
      test (boolean): whether to create a test set or not.

      Returns: 3-tuple of DataFrame, DataFrame, DataFrame (train, validation,
      test). If `test` is False, the third element of the 3-tuple is None.

      """
      n = len(df)

      n_val = int(split * n)
      n_test = int(split * n) if test else 0
      n_train = n - (n_val + n_test)

      idx = np.arange(n)
      if isinstance(seed, numbers.Number):
          np.random.seed(seed)
      np.random.shuffle(idx)

      df_shuffled = df.iloc[idx]

      df_train = df_shuffled.iloc[:n_train].copy()
      df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()
      df_test = df_shuffled.iloc[n_train+n_val:].copy() if test else None

      return df_train, df_val, df_test

#+end_src

Prepare the data. I add two arguments to =prepare_X=: =base=, which is a list of
the features in the dataframe that should be used, and =fns=, a list of
functions to extract additional features.

#+begin_src python
  def prepare_X(df, base, fns=[]):
      """Prepare a dataframe for learning.

      Convert the dataframe to a Numpy array:

      - Extract the features in `base`.

      - Apply the functions in `fns` to the dataframe to derive new features from
        existing ones (e.g., for binary encoding).

        The elements of `fns` should be lists `(fn, arg, arg, arg, ...)`. Before
        calling each function, `df` is prepended to the list of arguments. The
        functions should add the new features to `df`, and they should return a
        list of the names of the new feature(s) as strings.

      - Fill any missing data with 0.

      Note that `df` is not modified. The functions in `fns` should modify their
      dataframe argument, but they operate on a copy of `df`.

      Parameters:
      df (DataFrame): dataframe to convert.
      base (list of strings): list of fields in the dataframe to be used for the array.
      fns (list of tuples (function, arg list)): feature engineering functions.

      Returns:
      ndarray of the prepared data.

      """
      df = df.copy()
      features = base.copy()

      for fn, *args in fns:
          args = [df] + args
          new_features = fn(*args) # Note: `fn` should also modify the local copy of `df`!
          features += new_features

      df_num = df[features]
      df_num = df_num.fillna(0)
      X = df_num.values

      return X
#+end_src

Two functions for feature engineering:

#+begin_src python
  def binary_encode(df, feature, n=5):
      """Binary encode a categorical feature.

      Take the top `n` values of `feature` and add features to `df` to binary
      encode `feature`. The dataframe is modified in place.

      Parameters:
      df (DataFrame): the dataframe to add the feature to.
      feature (string): feature in df to be binary encoded.
      n (int): number of values for feature to encode.

      Returns:
      List of new features.

      """
      assert feature in df

      top_values = df[feature].value_counts().head(n)
      new_features = []
      for v in top_values.keys():
          binary_feature = feature + '_%s' % v
          df[binary_feature] = (df[feature] == v).astype(int)
          new_features.append(binary_feature)

      return new_features


  def encode_age(df, year_field, current_year):
      """Encode the age of an item as a feature.

      The age is calculated on the basis of the contents of `year_field` and
      `current_year`.

      Parameters:
      df (DataFrame): dataframe to encode the age in.
      year_feature (string): the feature that encodes the relevant year.
      current_year (int): the year used to calculate the age.

      Returns:
      Constant value ['age'].

      """
      assert year_field in df
      assert df[year_field].dtype == 'int64'

      df['age'] = current_year - df[year_field]

      return ['age']

#+end_src

=binary_encode= can be generalized to a function that loops over a list of
features:

#+begin_src python
  def binary_encodes(df, features, n=5):
      """Binary encode a list of features.

      Each feature is passed to `binary_encode`. See there for details. Note that
      `df` is modified in place.

      Parameters:
      df (DataFrame): the dataframe to engineer features from.
      features (list of strings): list of features to binary encode.
      n (int): number of values for feature to encode.

      Returns:
      A list of features added to `df`.

      """
      all_new_features = []
      for feature in features:
          new_features = binary_encode(df, feature, n)
          all_new_features += new_features

      return all_new_features

#+end_src

The =linear_regression= and =rmse= functions. These weren't modified:

#+begin_src python
  def linear_regression(X, y, r=0.0):
      """Perform linear regression.

      Parameters:
      X (ndarray): array of input values.
      y (ndarray): target values.
      r (float): regularization amount.

      Returns:
      Tuple of float, ndarray (bias, array of weights)

      """
      ones = np.ones(X.shape[0])
      X = np.column_stack([ones, X])

      XTX = X.T.dot(X)
      reg = r * np.eye(XTX.shape[0])
      XTX = XTX + reg

      XTX_inv = np.linalg.inv(XTX)
      w = XTX_inv.dot(X.T).dot(y)

      return w[0], w[1:]


  def rmse(y, y_pred):
      """Compute the root mean square error.

      Parameters:
      y (ndarray): target values.
      y_pred (ndarray): predicted values.

      Returns:
      float

      """
      error = y_pred - y
      mse = (error ** 2).mean()
      return np.sqrt(mse)

#+end_src


*** Car prices

The goal is to see if more feature engineering improves the model. The RMSE of
the model as developed in chapter 2 is 0.46. Can this be improved?

Let us set up the model. First, read the data and clean it up:

#+begin_src python
  df = read_data('../data/cars.csv')
  clean_alphanum_data(df)
  df.head()
#+end_src

#+RESULTS:
:   make       model  year             engine_fuel_type  engine_hp  engine_cylinders transmission_type  ...                        market_category  vehicle_size vehicle_style highway_mpg city_mpg  popularity   msrp
: 0  bmw  1_series_m  2011  premium_unleaded_(required)      335.0               6.0            manual  ...  factory_tuner,luxury,high-performance       compact         coupe          26       19        3916  46135
: 1  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                     luxury,performance       compact   convertible          28       19        3916  40650
: 2  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                luxury,high-performance       compact         coupe          28       20        3916  36350
: 3  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                     luxury,performance       compact         coupe          28       18        3916  29450
: 4  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                                 luxury       compact   convertible          28       18        3916  34500
:
: [5 rows x 16 columns]

Split the data set into a train, validation and test set:

#+begin_src python :results silent
  df_train, df_val, df_test = split_data_frame(df, split=0.2, seed=2)

  y_train = np.log1p(df_train.msrp.values)
  y_val = np.log1p(df_val.msrp.values)
  y_test = np.log1p(df_test.msrp.values)
#+end_src

Remove the target value ("msrp" or "manufacturer's suggested retail price") from
the data set:

#+begin_src python :results silent
  del df_train['msrp']
  del df_val['msrp']
  del df_test['msrp']
#+end_src

Prepare the data. To confirm the results in the book (and make sure my code is
working), I'll first use the same parameters:

#+begin_src python :results silent
  # Prepare the training data.
  base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']
  fns = [[encode_age, 'year', 2017],
         [binary_encodes, ["number_of_doors",
                           "make",
                           "engine_fuel_type",
                           "transmission_type",
                           "driven_wheels",
                           "market_category",
                           "vehicle_size",
                           "vehicle_style"],
          5]]
  X_train = prepare_X(df_train, base, fns)

#+end_src

Now train the model:

#+begin_src python :results silent
  w_0, w = linear_regression(X_train, y_train, 0.01)
#+end_src

If we apply the model to the training data, we *should* get the original prices
again. In reality, we don't.

#+begin_src python :results silent
  from matplotlib import pyplot as plt
  import seaborn as sns
#+end_src

#+begin_src python :results file figure/figure2-06.png
  y_pred = w_0 + X_train.dot(w)
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-06.png')
  'figures/figure2-06.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-06.png]]

We can compute the RMSE for the model:

#+begin_src python
  rmse(y_train, y_pred)
#+end_src

#+RESULTS:
: 0.46020995201980425

We should of course compute the RMSE on the validation set:

#+begin_src python
  X_val = prepare_X(df_val, base, fns)
  y_pred = w_0 + X_val.dot(w)
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 0.476510145790575

**** Using more values

Let's follow the suggestion in exercise 2.5.1 and include more values in the
binary encoded features:

#+begin_src python :results file figures/figure2-07.png
  base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']
  fns = [[encode_age, 'year', 2017],
         [binary_encodes, ["number_of_doors",
                           "make",
                           "engine_fuel_type",
                           "transmission_type",
                           "driven_wheels",
                           "market_category",
                           "vehicle_size",
                           "vehicle_style"],
          8]]

  X_train = prepare_X(df_train, base, fns)

  w_0, w = linear_regression(X_train, y_train, 0.01)

  y_pred = w_0 + X_train.dot(w)

  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-07.png')
  'figures/figure2-07.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-07.png]]

Evaluating against the validation set:

#+begin_src python
  X_val = prepare_X(df_val, base, fns)
  y_pred = w_0 + X_val.dot(w)
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 0.4850113356908553

The performance seems to have degraded, not improved, although only by a little.

Note that trying to use 10 values for binary encoding fails, because the
validation set then gains an extra feature. The error reported is:

#+begin_example
  ValueError: shapes (2382,61) and (60,) not aligned: 61 (dim 1) != 60 (dim 0)
#+end_example

I assume that in the validation set, one of the features has one value more than
in the training set.

**** Differentiating more                                       :noexport:

Among the binary encoded features, some have only a few values, others have a
larger number. Cutting them all off at the same point may not be a good idea.

These are the available features, their types and their value counts:

#+begin_src python
  features = []
  for column in df.columns:
      feature = [column, df[column].dtype.name, len(df[column].value_counts())]
      print(feature)
      features.append(feature)

  features
#+end_src

#+RESULTS:
| make              | object  |   48 |
| model             | object  |  914 |
| year              | int64   |   28 |
| engine_fuel_type  | object  |   10 |
| engine_hp         | float64 |  356 |
| engine_cylinders  | float64 |    9 |
| transmission_type | object  |    5 |
| driven_wheels     | object  |    4 |
| number_of_doors   | float64 |    3 |
| market_category   | object  |   71 |
| vehicle_size      | object  |    3 |
| vehicle_style     | object  |   16 |
| highway_mpg       | int64   |   59 |
| city_mpg          | int64   |   69 |
| popularity        | int64   |   48 |
| msrp              | int64   | 6049 |

Let's check which ones were used in the model above:

| feature           | type    | n values | used |
|-------------------+---------+----------+------|
| make              | object  |       48 | *    |
| model             | object  |      914 |      |
| year              | int64   |       28 |      |
| engine_fuel_type  | object  |       10 | *    |
| engine_hp         | float64 |      356 | *    |
| engine_cylinders  | float64 |        9 | *    |
| transmission_type | object  |        5 | *    |
| driven_wheels     | object  |        4 | *    |
| number_of_doors   | float64 |        3 | *    |
| market_category   | object  |       71 | *    |
| vehicle_size      | object  |        3 | *    |
| vehicle_style     | object  |       16 | *    |
| highway_mpg       | int64   |       59 | *    |
| city_mpg          | int64   |       69 | *    |
| popularity        | int64   |       48 | *    |
| msrp              | int64   |     6049 |      |

Not sure if any of these could be engineered better. Let's get on with the next
exercise.

*** House prices

First read the data and clean it up.

#+begin_src python
  df = read_data('../data/housing.csv')
  clean_alphanum_data(df)
  df.head()
#+end_src

#+RESULTS:
:       crim    zn  indus  chas    nox     rm   age     dis  rad    tax  ptratio       b  lstat  medv
: 0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0     15.3  396.90   4.98  24.0
: 1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0     17.8  396.90   9.14  21.6
: 2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0     17.8  392.83   4.03  34.7
: 3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0     18.7  394.63   2.94  33.4
: 4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0     18.7  396.90   5.33  36.2

The header uses abbreviations:

| CRIM    | per capita crime rate by town                                         |
| ZN      | proportion of residential land zoned for lots over 25,000 sq.ft.      |
| INDUS   | proportion of non-retail business acres per town                      |
| CHAS    | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |
| NOX     | nitric oxides concentration (parts per 10 million)                    |
| RM      | average number of rooms per dwelling                                  |
| AGE     | proportion of owner-occupied units built prior to 1940                |
| DIS     | weighted distances to five Boston employment centres                  |
| RAD     | index of accessibility to radial highways                             |
| TAX     | full-value property-tax rate per $10,000                              |
| PTRATIO | pupil-teacher ratio by town                                           |
| B       | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town        |
| LSTAT   | % lower status of the population                                      |
| MEDV    | Median value of owner-occupied homes in $1000’s                       |

#+begin_src python
  df.dtypes
#+end_src

#+RESULTS:
#+begin_example
crim       float64
zn         float64
indus      float64
chas         int64
nox        float64
rm         float64
age        float64
dis        float64
rad          int64
tax        float64
ptratio    float64
b          float64
lstat      float64
medv       float64
dtype: object
#+end_example

The target value is the MEDV. All values except CHAS are numeric, CHAS is
already binary, as it only contains the values 0 and 1. No preprocessing seems
to be necessary.

There are no missing values in the data set:

#+begin_src python
  df.isnull().sum()
#+end_src

#+RESULTS:
#+begin_example
crim       0
zn         0
indus      0
chas       0
nox        0
rm         0
age        0
dis        0
rad        0
tax        0
ptratio    0
b          0
lstat      0
medv       0
dtype: int64
#+end_example

Let's first plot the data:

#+begin_src python :results silent
  from matplotlib import pyplot as plt
  import seaborn as sns
#+end_src

#+begin_src python :results file figures/figure2-09.png
  plt.clf()
  sns.histplot(df.medv, kde=True)
  plt.savefig('figures/figure2-09.png')

  'figures/figure2-09.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-09.png]]

Applying logarithm transformation:

#+begin_src python :results file figures/figure2-10.png
  plt.clf()
  log_price = np.log1p(df.medv)
  sns.histplot(log_price, kde=True)
  plt.savefig('figures/figure2-10.png')

  'figures/figure2-10.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-10.png]]

This doesn't seem to change much. Let's try both ways and see if one comes out
better.

**** No log transformation

Since the data set isn't that big (only 506 items) and we're only playing
around, let's forego creating a test set.

#+begin_src python :results silent
  df_train, df_val, _ = split_data_frame(df, split=0.2, seed=2, test=False)

  y_train = df_train.medv.values
  y_val = df_val.medv.values

  del df_train['medv']
  del df_val['medv']
#+end_src

Prepare the training data:

#+begin_src python :results silent
  base = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']
  X_train = prepare_X(df_train, base)
#+end_src

Train the network:

#+begin_src python :results silent
  w_0, w = linear_regression(X_train, y_train, 0.01)
#+end_src

Plot the results:

#+begin_src python :results file figure/figure2-11.png
  y_pred = w_0 + X_train.dot(w)
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-11.png')
  'figures/figure2-11.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-11.png]]

Not that bad, but too high in the middle-high segment. Let's do validation:

#+begin_src python
  X_val = prepare_X(df_val, base)
  y_pred = w_0 + X_val.dot(w)
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 3.6761997561821307

Ugh.

**** With log transformation

Now let's apply logarithm transformation;

#+begin_src python :results silent
  df_train, df_val, _ = split_data_frame(df, split=0.2, seed=2, test=False)

  y_train = np.log1p(df_train.medv.values)
  y_val = np.log1p(df_val.medv.values)

  del df_train['medv']
  del df_val['medv']
#+end_src

Prepare the training data:

#+begin_src python :results silent
  base = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']
  X_train = prepare_X(df_train, base)
#+end_src

Train the network:

#+begin_src python :results silent
  w_0, w = linear_regression(X_train, y_train, 0.01)
#+end_src

Plot the results:

#+begin_src python :results file figure/figure2-12.png
  y_pred = w_0 + X_train.dot(w)
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-12.png')
  'figures/figure2-12.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-12.png]]

At first sight, not much better than without log transformation. But let's see:

#+begin_src python
  X_val = prepare_X(df_val, base)
  y_pred = w_0 + X_val.dot(w)
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 0.164499584999961

That's a much better result, it seems.


* Chapter 3: Machine Learning for Classification

** Notes to chapter 3                                             :noexport:
:PROPERTIES:
:header-args:python+: :session ch3-notes
:END:

*** Imports

#+begin_src python :results silent
  import numpy as np
  import pandas as pd

  import seaborn as sns
  from matplotlib import pyplot as plt
#+end_src

*** Initial look at the data

#+begin_src python
  df = pd.read_csv('../data/telco-customer-churn.csv')
  df.head()
#+end_src

#+RESULTS:
:    customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService     MultipleLines  ... StreamingTV StreamingMovies        Contract PaperlessBilling              PaymentMethod MonthlyCharges TotalCharges Churn
: 0  7590-VHVEG  Female              0     Yes         No       1           No  No phone service  ...          No              No  Month-to-month              Yes           Electronic check          29.85        29.85    No
: 1  5575-GNVDE    Male              0      No         No      34          Yes                No  ...          No              No        One year               No               Mailed check          56.95       1889.5    No
: 2  3668-QPYBK    Male              0      No         No       2          Yes                No  ...          No              No  Month-to-month              Yes               Mailed check          53.85       108.15   Yes
: 3  7795-CFOCW    Male              0      No         No      45           No  No phone service  ...          No              No        One year               No  Bank transfer (automatic)          42.30      1840.75    No
: 4  9237-HQITU  Female              0      No         No       2          Yes                No  ...          No              No  Month-to-month              Yes           Electronic check          70.70       151.65   Yes
:
: [5 rows x 21 columns]

#+begin_src python
  df.head().T
#+end_src

#+RESULTS:
#+begin_example
                                 0             1               2                          3                 4
customerID              7590-VHVEG    5575-GNVDE      3668-QPYBK                 7795-CFOCW        9237-HQITU
gender                      Female          Male            Male                       Male            Female
SeniorCitizen                    0             0               0                          0                 0
Partner                        Yes            No              No                         No                No
Dependents                      No            No              No                         No                No
tenure                           1            34               2                         45                 2
PhoneService                    No           Yes             Yes                         No               Yes
MultipleLines     No phone service            No              No           No phone service                No
InternetService                DSL           DSL             DSL                        DSL       Fiber optic
OnlineSecurity                  No           Yes             Yes                        Yes                No
OnlineBackup                   Yes            No             Yes                         No                No
DeviceProtection                No           Yes              No                        Yes                No
TechSupport                     No            No              No                        Yes                No
StreamingTV                     No            No              No                         No                No
StreamingMovies                 No            No              No                         No                No
Contract            Month-to-month      One year  Month-to-month                   One year    Month-to-month
PaperlessBilling               Yes            No             Yes                         No               Yes
PaymentMethod     Electronic check  Mailed check    Mailed check  Bank transfer (automatic)  Electronic check
MonthlyCharges               29.85         56.95           53.85                       42.3              70.7
TotalCharges                 29.85        1889.5          108.15                    1840.75            151.65
Churn                           No            No             Yes                         No               Yes
#+end_example

#+begin_src python
  df.dtypes
#+end_src

#+RESULTS:
#+begin_example
customerID           object
gender               object
SeniorCitizen         int64
Partner              object
Dependents           object
tenure                int64
PhoneService         object
MultipleLines        object
InternetService      object
OnlineSecurity       object
OnlineBackup         object
DeviceProtection     object
TechSupport          object
StreamingTV          object
StreamingMovies      object
Contract             object
PaperlessBilling     object
PaymentMethod        object
MonthlyCharges      float64
TotalCharges         object
Churn                object
dtype: object
#+end_example

Note that 'TotalCharges' is an object, even though we'd expect it to be a
number. There are probably non-numeric characters in this field:

#+begin_src python
  total_charges = pd.to_numeric(df['TotalCharges'], errors='coerce')  # Convert to number, do not fail on errors.
  df[total_charges.isnull()][['customerID', 'TotalCharges']]          # List those where TotalCharges is empty.
#+end_src

#+RESULTS:
#+begin_example
      customerID TotalCharges
488   4472-LVYGI
753   3115-CZMZD
936   5709-LVOEQ
1082  4367-NUYAO
1340  1371-DWPAZ
3331  7644-OMVMY
3826  3213-VVOLG
4380  2520-SGTTA
5218  2923-ARZLG
6670  4075-WKNIU
6754  2775-SEFEE
#+end_example

This shows that there are some entries where "TotalCharges" contains whitespace.

* COMMENT Local Variables
:PROPERTIES:
:VISIBILITY: folded
:END:
# Local Variables:
# eval: (guess-language-mode -1)
# ispell-local-dictionary: "english"
# eval: (visual-fill-column-mode -1)
# eval: (auto-fill-mode 1))
# eval: (hl-line-mode 1)
# eval: (auto-revert-mode 1)
# eval: (mixed-pitch-mode -1)
# End:
