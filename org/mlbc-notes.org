#+TITLE: Notes on "Machine Learning Bookcamp"
#+PROPERTY: header-args:python :exports both :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="styles/readtheorg_theme/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="styles/readtheorg_theme/css/readtheorg.css"/>
#+HTML_HEAD: <script type="text/javascript" src="styles/lib/js/jquery.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="styles/lib/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="styles/lib/js/jquery.stickytableheaders.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="styles/readtheorg_theme/js/readtheorg.js"></script>

* Introduction


* Setup                                                            :noexport:

Run the code in a virtual environment:

#+begin_src emacs-lisp :results silent
  (pyvenv-workon "mlbc-JThvy3A1-py3.8")
#+end_src


* Chapter 2: Machine Learning for Regression

** Notes to chapter 2                                             :noexport:
:PROPERTIES:
:header-args:python+: :session ch2-notes
:END:

*** Imports

#+begin_src python :results silent
  import pandas as pd
  import numpy as np

  from matplotlib import pyplot as plt
  import seaborn as sns
#+end_src

*** Reading and preparing the data

#+begin_src python :results value
  df = pd.read_csv('../data/cars.csv')
  len(df)
#+end_src

#+RESULTS:
: 11914

#+begin_src python
  df.head()
#+end_src

#+RESULTS:
:   Make       Model  Year             Engine Fuel Type  ...  highway MPG  city mpg Popularity   MSRP
: 0  BMW  1 Series M  2011  premium unleaded (required)  ...           26        19       3916  46135
: 1  BMW    1 Series  2011  premium unleaded (required)  ...           28        19       3916  40650
: 2  BMW    1 Series  2011  premium unleaded (required)  ...           28        20       3916  36350
: 3  BMW    1 Series  2011  premium unleaded (required)  ...           28        18       3916  29450
: 4  BMW    1 Series  2011  premium unleaded (required)  ...           28        18       3916  34500
:
: [5 rows x 16 columns]

#+begin_src python
  df.dtypes
#+end_src

#+RESULTS:
#+begin_example
Make                  object
Model                 object
Year                   int64
Engine Fuel Type      object
Engine HP            float64
Engine Cylinders     float64
Transmission Type     object
Driven_Wheels         object
Number of Doors      float64
Market Category       object
Vehicle Size          object
Vehicle Style         object
highway MPG            int64
city mpg               int64
Popularity             int64
MSRP                   int64
dtype: object
#+end_example

Cleaning up the data:

#+begin_src python :results silent
  df.columns = df.columns.str.lower().str.replace(' ', '_')

  string_columns = list(df.dtypes[df.dtypes == 'object'].index)

  for col in string_columns:
      df[col] = df[col].str.lower().str.replace(' ', '_')
#+end_src

#+begin_src python
  df.head()
#+end_src

#+RESULTS:
:   make       model  year             engine_fuel_type  engine_hp  engine_cylinders transmission_type  ...                        market_category  vehicle_size vehicle_style highway_mpg city_mpg  popularity   msrp
: 0  bmw  1_series_m  2011  premium_unleaded_(required)      335.0               6.0            manual  ...  factory_tuner,luxury,high-performance       compact         coupe          26       19        3916  46135
: 1  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                     luxury,performance       compact   convertible          28       19        3916  40650
: 2  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                luxury,high-performance       compact         coupe          28       20        3916  36350
: 3  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                     luxury,performance       compact         coupe          28       18        3916  29450
: 4  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                                 luxury       compact   convertible          28       18        3916  34500
:
: [5 rows x 16 columns]

- Notes:
  - =df.dtypes= gives a list of types, =df.dtypes[df.dtypes == 'object']= lists
    only those that have the given type.
  - =df.dtypes.index= gives an Index object listing all the relevant columns.
  - The =str= attribute makes it possible to apply string operations to all the
    elements in the column at once.

#+begin_src python :results file figures/figure2-01.png
  sns.displot(df.msrp, kde=False)
  plt.savefig('figures/figure2-01.png')
  'figures/figure2-01.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-01.png]]

#+begin_src python :results file figures/figure2-02.png
  sns.displot(df.msrp[df.msrp < 100000], kde=False)
  plt.savefig('figures/figure2-02.png')
  'figures/figure2-02.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-02.png]]

This kind of distribution is difficult for machine learning algorithms, esp.
linear regression, because of the long tail of high prices, which occur
relatively rarely, but must still be learned.

The common solution in such cases is to apply a logarithm transformation to the
*target value*:

y_{new} = log(y+1)

Adding 1 to the original target value avoids calculating log(0) = -∞. Numpy has
a function for this purpose, =np.log1p=:

#+begin_src python :results file figures/figure2-03.png
  log_price = np.log1p(df.msrp)
  sns.displot(log_price)
  plt.savefig('figures/figure2-03.png')
  'figures/figure2-03.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-03.png]]

This so-called "normal or Gaussian distribution" is more amenable to machine
learning algorithms.

Note that there are a lot of empty cells in the dataframe. These need to be
dealt with (see below):

#+begin_src python
  df.isnull().sum()
#+end_src

#+RESULTS:
#+begin_example
make                    0
model                   0
year                    0
engine_fuel_type        3
engine_hp              69
engine_cylinders       30
transmission_type       0
driven_wheels           0
number_of_doors         6
market_category      3742
vehicle_size            0
vehicle_style           0
highway_mpg             0
city_mpg                0
popularity              0
msrp                    0
dtype: int64
#+end_example

*** Setting up the validation framework

*** Splitting the data into a train, a validation and a test set
- 20% for validation
- 20% for testing
- 60% for training

#+begin_src python :results silent
  n = len(df)

  n_val = int(0.2 * n)
  n_test = int(0.2 * n)
  n_train = n - (n_val + n_test)

  np.random.seed(2)
  idx = np.arange(n)
  np.random.shuffle(idx)

  df_shuffled = df.iloc[idx]

  df_train = df_shuffled.iloc[:n_train].copy()
  df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()
  df_test = df_shuffled.iloc[n_train+n_val:].copy()
#+end_src

We still need to apply the log transformation:

#+begin_src python :results silent
  y_train = np.log1p(df_train.msrp.values)
  y_val = np.log1p(df_val.msrp.values)
  y_test = np.log1p(df_test.msrp.values)
#+end_src

The target value should be removed from the dataframes, just in case:

#+begin_src python :results silent
  del df_train['msrp']
  del df_val['msrp']
  del df_test['msrp']
#+end_src

*** Training the model

**** Linear regression

Computing the weights =w= can be done with the "normal equation":

w = (X^{T}·X)^{-1}·X^{T}·y

where:

- X is a matrix of input features
- y is a vector of target values
- X^{T} is the *transpose* of X (=X.T= in Numpy)
- X^{-1} is the *inverse* of X (=np.linalg.inv= in Numpy)


The dot product in Numpy is obtained with the =dot()= method. Thus, the formula
above becomes:

#+begin_src python
w = inv(X.T.dot(X)).dot(X.T).dot(y)
#+end_src

**** Implementing the normal equation

In Python:

#+begin_src python :results silent
  def linear_regression(X, y):
      # X: matrix of features
      # y: vector of target values

      # Add a dummy column to accommodate the bias.
      ones = np.ones(X.shape[0])
      X = np.column_stack([ones, X])

      # Normal equation formula
      XTX = X.T.dot(X)
      XTX_inv = np.linalg.inv(XTX)
      w = XTX_inv.dot(X.T).dot(y)

      # Split the bias and the weights
      return w[0], w[1:]
#+end_src

*** Predicting the price: baseline solution

We select a few features to illustrate how things work:

#+begin_src python
  base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']
  df_num = df_train[base]
  df_num.head()
#+end_src

#+RESULTS:
:        engine_hp  engine_cylinders  highway_mpg  city_mpg  popularity
: 2735       148.0               4.0           33        24        1385
: 6720       132.0               4.0           32        25        2031
: 5878       148.0               4.0           37        28         640
: 11190       90.0               4.0           18        16         873
: 4554       385.0               8.0           21        15        5657

Replace any missing values with 0:

#+begin_src python :results silent
  df_num = df_num.fillna(0)
#+end_src

This may not be the best way to deal with missing values, but it works.

#+begin_remark
I guess what's not so great about it is that it reduces a term to zero in the
equation, causing the predicted price to be lower than one might expect. This is
the formula for predicting the price:

g(x) = w_{0} + x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3} + ...

Now if one feature is set to 0, the total sum g(x) is lower than it would have
been if the feature were not 0. A better solution might be to set unknown
features to the mean of that feature across all samples. That way the feature
still exerts its influence on the total price.

For example, if =city_mpg= is unknown, we may still assume that it isn't zero.
Setting it to zero would drive down the estimated price unreasonably. (Or drive
it up, depending on the relevant weight.)
#+end_remark

Convert the data frame to a Numpy array. This is an important step, as the
data frame cannot be fed to the function =linear_regression=:

#+begin_src python :results silent
  X_train = df_num.values
#+end_src

Now train the model:

#+begin_src python :results silent
  w_0, w = linear_regression(X_train, y_train)
#+end_src

#+begin_remark
Note: Training the model means calculating the weights (well, duh!) Here, the
weights can simply be calculated, but formally it's still training.
#+end_remark

Applying the model to the training data:

#+begin_src python :results silent
  y_pred = w_0 + X_train.dot(w)
#+end_src

And plot the result:

#+begin_src python :results file figures/figure2-04.png
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-04.png')
  'figures/figure2-04.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-04.png]]

Note: the predicted and the actual values are quite a bit apart. This is due to
the fact that the predictions here are based on only five features.

#+begin_remark
The book uses =sns.distplot()=, which however gives a deprecation warning. One
should use =sns.displot()= or =sns.histplot()= instead, but only the latter
seems to allow overlaying two plots.
#+end_remark

*** Evaluating the model: Root Mean Square Error

The Root Mean Square Error (RMSE) is a common measure for the quality of a
model:

\[
\mathrm{RMSE} = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(g(x_{i})-y_{i})^{2}}
\]

RMSE in Python using Numpy:

#+begin_src python :results silent
  def rmse(y, y_pred):
      # y: array of target values
      # y_pred: array of predicted values

      error = y_pred - y
      mse = (error ** 2).mean()
      return np.sqrt(mse)
#+end_src

Note: Numpy does array operations. =y= and =y_pred= are arrays, which means that
=error= is, as well.

Computing the RMSE for the current model:

#+begin_src python
  rmse(y_train, y_pred)
#+end_src

#+RESULTS:
: 0.7554192603920132

To compare the model with others, this measure should be computed on the
validation set, not the training set:

#+begin_src python
  # Create the matrix of validation samples X_val:
  df_num = df_val[base]
  df_num = df_num.fillna(0)
  X_val = df_num.values

  # Apply the model:
  y_pred = w_0 + X_val.dot(w)

  # Compute RMSE;
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 0.7616530991301577

To make this more easily repeatable:

#+begin_src python :results silent
  def prepare_X(df):
      df_num = df[base]
      df_num = df_num.fillna(0)
      X = df_num.values

      return X
#+end_src

=prepare_X= creates a matrix from a data frame. Training and evaluation are now
simpler:

#+begin_src python :results output
  X_train = prepare_X(df_train)
  w_0, w = linear_regression(X_train, y_train)

  X_val = prepare_X(df_val)
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))
#+end_src

#+RESULTS:
: validation: 0.7616530991301577

*** Simple feature engineering

We can add new features based on the existing features. For example, the year a
car is produced is only a good predictor of price if it's interpreted as the age
of a car.

=df_train.year.max()= gives the newest car in the data set, which is 2017.
Subtract the year of a car from 2017 to get its age.

#+begin_src python :results silent
  def prepare_X(df):
      df = df.copy()
      features = base.copy()

      df['age'] = 2017 - df.year
      features.append('age')

      df_num = df[features]
      df_num = df_num.fillna(0)
      X = df_num.values

      return X
#+end_src

Training and evaluation can now be done as follows:

#+begin_src python :results output
  X_train = prepare_X(df_train)                    # Prepare the data.
  w_0, w = linear_regression(X_train, y_train)     # Training the model.

  X_val = prepare_X(df_val)                        # Apply the model to the validation set.
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
#+end_src

#+RESULTS:
: validation: 0.5172055461058327

Plotting the distribution of the predicted values:

#+begin_src python :results file figures/figure2-05.png
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_val, label='y', color="red")
  plt.legend()

  plt.savefig('figures/figure2-05.png')
  'figures/figure2-05.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-05.png]]

*** Categorical features

Categorical features are features that take one of a limited set of values.
These are often strings, but may be numerical, as the number of doors of a car
(2, 3, or 4).

One way to handle categorical features in a model is to include a set of binary
features, one for each distinct value (called *one-hot encoding*). We can do
this in the =prepare_X= function:

#+begin_src python :results silent
  def prepare_X(df):
      # Copy the data frame and the features.
      df = df.copy()
      features = base.copy()

      # Add some features
      df['age'] = 2017 - df.year
      features.append('age')

      for v in [2, 3, 4]:
          feature = 'num_doors_%s' % v
          value = (df['number_of_doors'] == v).astype(int)
          df[feature] = value
          features.append(feature)

      # Create a new data frame with only the features and add any missing features as 0.
      df_num = df[features]
      df_num = df_num.fillna(0)

      # Extract the values into a matrix and return the result.
      X = df_num.values
      return X
#+end_src

#+begin_remark
I'm not sure when a comparison can be turned into an integer...
=True.astype(int)= returns an error, and so does =(1==0).astype(int)=, but for
some reason, =(df['number_of_doors'][0]==2).astype(int)= returns =1=.

Note that =(df['number_of_doors']==v)= is an array operation: it returns a
Pandas series of boolean values.
#+end_remark

Doing the same for the feature make, taking only the five most frequently
occurring values:

#+begin_src python :results silent
  def prepare_X(df):
      # Copy the data frame and the features.
      df = df.copy()
      features = base.copy()

      # Add some features
      df['age'] = 2017 - df.year
      features.append('age')

      for v in [2, 3, 4]:
          feature = 'num_doors_%s' % v
          value = (df['number_of_doors'] == v).astype(int)
          df[feature] = value
          features.append(feature)

      for v in ["chevrolet", "ford", "volkswagen", "toyota", "dodge"]:
          feature = 'is_make_%s' % v
          df[feature] = (df['make'] == v).astype(int)
          features.append(feature)

      # Create a new data frame with only the features and add any missing features as 0.
      df_num = df[features]
      df_num = df_num.fillna(0)

      # Extract the values into a matrix and return the result.
      X = df_num.values
      return X
#+end_src

See if it works:

#+begin_src python :results output
  X_train = prepare_X(df_train)                    # Prepare the data.
  w_0, w = linear_regression(X_train, y_train)     # Training the model.

  X_val = prepare_X(df_val)                        # Apply the model to the validation set.
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
#+end_src

#+RESULTS:
: validation: 0.5076038849556838

Adding some more categorical features:

#+begin_src python :results silent
  def prepare_X(df):
      # Copy the data frame and the features.
      df = df.copy()
      features = base.copy()

      # Add some features
      df['age'] = 2017 - df.year
      features.append('age')

      for v in [2, 3, 4]:
          feature = 'num_doors_%s' % v
          value = (df['number_of_doors'] == v).astype(int)
          df[feature] = value
          features.append(feature)

      for v in ["chevrolet", "ford", "volkswagen", "toyota", "dodge"]:
          feature = 'is_make_%s' % v
          df[feature] = (df['make'] == v).astype(int)
          features.append(feature)

      for v in ['regular_unleaded', 'premium_unleaded_(required)',
                'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']:
          feature = 'is_type_%s' % v
          df[feature] = (df['engine_fuel_type'] == v).astype(int)
          features.append(feature)

      for v in ['automatic', 'manual', 'automated_manual']:
          feature = 'is_transmission_%s' % v
          df[feature] = (df['transmission_type'] == v).astype(int)
          features.append(feature)

      for v in ['front_wheel_drive', 'rear_wheel_drive',
                'all_wheel_drive', 'four_wheel_drive']:
          feature = 'is_driven_wheels_%s' % v
          df[feature] = (df['driven_wheels'] == v).astype(int)
          features.append(feature)

      for v in ['crossover', 'flex_fuel', 'luxury', 'luxury,performance', 'hatchback']:
          feature = 'is_mc_%s' % v
          df[feature] = (df['market_category'] == v).astype(int)
          features.append(feature)

      for v in ['compact', 'midsize', 'large']:
          feature = 'is_size_%s' % v
          df[feature] = (df['vehicle_size'] == v).astype(int)
          features.append(feature)

      for v in ['sedan', '4dr_suv', 'coupe', 'convertible', '4dr_hatchback']:
          feature = 'is_style_%s' % v
          df[feature] = (df['vehicle_style'] == v).astype(int)
          features.append(feature)

      # Create a new data frame with only the features and add any missing features as 0.
      df_num = df[features]
      df_num = df_num.fillna(0)

      # Extract the values into a matrix and return the result.
      X = df_num.values
      return X
#+end_src

Checking out the effect:

#+begin_src python :results output
  X_train = prepare_X(df_train)                    # Prepare the data.
  w_0, w = linear_regression(X_train, y_train)     # Train the model.

  X_val = prepare_X(df_val)                        # Apply the model to the validation set.
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))        # Compute RMSE of the validation data
#+end_src

#+RESULTS:
: validation: 22.322123465036622

Adding these features makes the model much worse, not better.

*** Regularization

The reason for the deterioration is *numerical instability*. The bias is very
large and so are some of the weights:

#+begin_src python :results output
  print('bias: %s\nweights: %s' % (w_0, w))
#+end_src

#+RESULTS:
#+begin_example
bias: 8991164041495205.0
weights: [-4.95981777e-02  6.73670308e+00  9.40777511e-01 -2.58497309e+00
  3.72822950e-03 -5.20036150e-01 -1.40699123e+03 -1.39430142e+03
 -1.39683545e+03 -5.60490940e+00 -2.27794179e+01  1.73041774e+01
 -4.30960052e+00 -9.23053458e+00  5.11353883e+01  5.56224498e+01
  4.86289752e+01  5.76621679e+01 -2.18304488e+02 -2.07996848e+02
 -2.72177915e+02 -8.99116404e+15 -8.99116404e+15 -8.99116404e+15
 -8.99116404e+15  6.13723252e+00  6.05470595e+00 -1.21844079e+00
  3.04348851e+00  1.20577703e+00 -2.16182997e+01 -2.60265778e+01
 -2.38977036e+01 -7.67460186e-02  4.14645821e-02  1.86187511e-01
  3.55798979e-01 -2.14066472e-01]
#+end_example

The underlying cause of the problem is that the feature matrix becomes
*singular* or *undetermined*. This can happen when two features are essentially
the same, e.g., if there's a feature "miles per gallon" and you then add a
feature "kilometers per liter".

Technically, the matrix produced here is not singular, but the large bias and
weights indicate it's close.

This numerical instability can be solved using *regularization* techniques. One
way to do regularization is to add a small number to each diagonal element of
the matrix. The formula for linear regression then becomes:

w = (X^{T}·X+αI)^{-1}·X^{T}·y

I is an identity matrix, α a constant. In Numpy:

#+begin_example
  XTX = X_train.T.dot(X_train)
  XTX = XTX + 0.01 * np.eye(XTX.shape[0])
#+end_example

Here, \alpha is set to =0.01=. The function =np.eye()= creates a 2D identity matrix:

#+begin_src python
  0.01 * np.eye(4)
#+end_src

#+RESULTS:
| 0.01 |    0 |    0 |    0 |
|    0 | 0.01 |    0 |    0 |
|    0 |    0 | 0.01 |    0 |
|    0 |    0 |    0 | 0.01 |

Linear regression with regularization:

#+begin_src python :results silent
  def linear_regression_reg(X, y, r=0.01):
      ones = np.ones(X.shape[0])
      X = np.column_stack([ones, X])

      XTX = X.T.dot(X)
      reg = r * np.eye(XTX.shape[0])
      XTX = XTX + reg

      XTX_inv = np.linalg.inv(XTX)
      w = XTX_inv.dot(X.T).dot(y)

      return w[0], w[1:]
#+end_src

A grid search suggests that values around 0.01 are fine. Smaller values do
reduce the RMSE, but only marginally.

#+begin_src python :results output
  X_train = prepare_X(df_train)
  w_0, w = linear_regression_reg(X_train, y_train, r=0.01)

  X_val = prepare_X(df_val)
  y_pred = w_0 + X_val.dot(w)
  print('validation:', rmse(y_val, y_pred))

  X_test = prepare_X(df_test)
  y_pred = w_0 + X_test.dot(w)
  print('test:', rmse(y_test, y_pred))
#+end_src

#+RESULTS:
: validation: 0.46023949630840544
: test: 0.45718136795913034

The results suggest that the model works well.

#+begin_remark
Still, I'm not clear on whether the value of approx. 0.46 is good or not. Does
it mean the model predicts the price well or not?
#+end_remark

*** Using the model

When using the model to make a prediction, one needs to create a data frame with
one row. Take the following ad for a car:

#+begin_src python :results silent
  ad = {
      'city_mpg'          : 18,
      'driven_wheels'     : 'all_wheel_drive',
      'engine_cylinders'  : 6.0,
      'engine_fuel_type'  : 'regular_unleaded',
      'engine_hp'         : 268.0,
      'highway_mpg'       : 25,
      'make'              : 'toyota',
      'market_category'   : 'crossover,performance',
      'model'             : 'venza',
      'number_of_doors'   : 4.0,
      'popularity'        : 2031,
      'transmission_type' : 'automatic',
      'vehicle_size'      : 'midsize',
      'vehicle_style'     : 'wagon',
      'year'              : 2013
  }
#+end_src

Converting this to a data frame and a matrix:

#+begin_src python :results silent
  df_ad = pd.DataFrame([ad])
  X_test = prepare_X(df_ad)
#+end_src

Applying the model yields a value that is the logarithm of the predicted price.
To calculate the price, apply the exponent function:

#+begin_src python :results output
  y_pred = w_0 + X_test.dot(w)
  suggestion = np.expm1(y_pred)
  print('suggested price: $%d' % round(suggestion[0]))
#+end_src

#+RESULTS:
: suggested price: $28294


** Utility functions
:PROPERTIES:
:header-args:python+: :tangle mlutils.py :results silent :eval no
:END:

Convert the code from chapter 2 to a set of utility functions. These are tangled
to a file =mlutils.py= in the current directory, which can then be imported in
code blocks below.

#+begin_src python
  import numbers

  import pandas as pd
  import numpy as np

#+end_src

Read the data and clean up the column names / alphanumeric data:

#+begin_src python
  def read_data(file):
      """Read a dataframe from a CSV file.

      Parameters:
      file (string): path to a CSV file.

      Returns:
      DataFrame holding the contents of the file.
      """
      df = pd.read_csv(file)
      return df


  def clean_alphanum_data(df):
      """Clean up alphanumeric data in a dataframe.

      Convert all strings to lower case and replace spaces with underscores.

      Parameters:
      df (DataFrame): the dataframe to be cleaned.

      Returns:
      None.
      """
      df.columns = df.columns.str.lower().str.replace(' ', '_')

      string_columns = list(df.dtypes[df.dtypes == 'object'].index)
      for col in string_columns:
          df[col] = df[col].str.lower().str.replace(' ', '_')

#+end_src

Split the data frame into a train, validation and test set:

#+begin_src python
  def split_data_frame(df, split=0.2, seed=None, test=True):
      """Split a dataframe into a train, validation and test set.

      The dataframe is first randomized and then split into three parts. If
      `validation` is False, the dataframe is split into two parts.

      Parameters:
      df (DataFrame): the dataframe to split.
      split (float):  fraction of the dataframe to use for validation and test sets.
      seed (int):     the seed used for randomization.
      test (boolean): whether to create a test set or not.

      Returns: 3-tuple of DataFrame, DataFrame, DataFrame (train, validation,
      test). If `test` is False, the third element of the 3-tuple is None.

      """
      n = len(df)

      n_val = int(split * n)
      n_test = int(split * n) if test else 0
      n_train = n - (n_val + n_test)

      idx = np.arange(n)
      if isinstance(seed, numbers.Number):
          np.random.seed(seed)
      np.random.shuffle(idx)

      df_shuffled = df.iloc[idx]

      df_train = df_shuffled.iloc[:n_train].copy()
      df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()
      df_test = df_shuffled.iloc[n_train+n_val:].copy() if test else None

      return df_train, df_val, df_test

#+end_src

Prepare the data. I add two arguments to =prepare_X=: =base=, which is a list of
the features in the dataframe that should be used, and =fns=, a list of
functions to extract additional features.

#+begin_src python
  def prepare_X(df, base, fns=[]):
      """Prepare a dataframe for learning.

      Convert the dataframe to a Numpy array:

      - Extract the features in `base`.

      - Apply the functions in `fns` to the dataframe to derive new features from
        existing ones (e.g., for binary encoding).

        The elements of `fns` should be lists `(fn, arg, arg, arg, ...)`. Before
        calling each function, `df` is prepended to the list of arguments. The
        functions should add the new features to `df`, and they should return a
        list of the names of the new feature(s) as strings.

      - Fill any missing data with 0.

      Note that `df` is not modified. The functions in `fns` should modify their
      dataframe argument, but they operate on a copy of `df`.

      Parameters:
      df (DataFrame): dataframe to convert.
      base (list of strings): list of fields in the dataframe to be used for the array.
      fns (list of tuples (function, arg list)): feature engineering functions.

      Returns:
      ndarray of the prepared data.

      """
      df = df.copy()
      features = base.copy()

      for fn, *args in fns:
          args = [df] + args
          new_features = fn(*args) # Note: `fn` should also modify the local copy of `df`!
          features += new_features

      df_num = df[features]
      df_num = df_num.fillna(0)
      X = df_num.values

      return X
#+end_src

Two functions for feature engineering:

#+begin_src python
  def binary_encode(df, feature, n=5):
      """Binary encode a categorical feature.

      Take the top `n` values of `feature` and add features to `df` to binary
      encode `feature`. The dataframe is modified in place.

      Parameters:
      df (DataFrame): the dataframe to add the feature to.
      feature (string): feature in df to be binary encoded.
      n (int): number of values for feature to encode.

      Returns:
      List of new features.

      """
      assert feature in df

      top_values = df[feature].value_counts().head(n)
      new_features = []
      for v in top_values.keys():
          binary_feature = feature + '_%s' % v
          df[binary_feature] = (df[feature] == v).astype(int)
          new_features.append(binary_feature)

      return new_features


  def encode_age(df, year_field, current_year):
      """Encode the age of an item as a feature.

      The age is calculated on the basis of the contents of `year_field` and
      `current_year`.

      Parameters:
      df (DataFrame): dataframe to encode the age in.
      year_feature (string): the feature that encodes the relevant year.
      current_year (int): the year used to calculate the age.

      Returns:
      Constant value ['age'].

      """
      assert year_field in df
      assert df[year_field].dtype == 'int64'

      df['age'] = current_year - df[year_field]

      return ['age']

#+end_src

=binary_encode= can be generalized to a function that loops over a list of
features:

#+begin_src python
  def binary_encodes(df, features, n=5):
      """Binary encode a list of features.

      Each feature is passed to `binary_encode`. See there for details. Note that
      `df` is modified in place.

      Parameters:
      df (DataFrame): the dataframe to engineer features from.
      features (list of strings): list of features to binary encode.
      n (int): number of values for feature to encode.

      Returns:
      A list of features added to `df`.

      """
      all_new_features = []
      for feature in features:
          new_features = binary_encode(df, feature, n)
          all_new_features += new_features

      return all_new_features

#+end_src

The =linear_regression= and =rmse= functions. These weren't modified:

#+begin_src python
  def linear_regression(X, y, r=0.0):
      """Perform linear regression.

      Parameters:
      X (ndarray): array of input values.
      y (ndarray): target values.
      r (float): regularization amount.

      Returns:
      Tuple of float, ndarray (bias, array of weights)

      """
      ones = np.ones(X.shape[0])
      X = np.column_stack([ones, X])

      XTX = X.T.dot(X)
      reg = r * np.eye(XTX.shape[0])
      XTX = XTX + reg

      XTX_inv = np.linalg.inv(XTX)
      w = XTX_inv.dot(X.T).dot(y)

      return w[0], w[1:]


  def rmse(y, y_pred):
      """Compute the root mean square error.

      Parameters:
      y (ndarray): target values.
      y_pred (ndarray): predicted values.

      Returns:
      float

      """
      error = y_pred - y
      mse = (error ** 2).mean()
      return np.sqrt(mse)

#+end_src


** Exercises and code
:PROPERTIES:
:header-args:python+: :session ch2-ex
:END:

*** Car prices

The goal is to see if more feature engineering improves the model. The RMSE of
the model as developed in chapter 2 is 0.46. Can this be improved?

Let us set up the model. First, read the data and clean it up:

#+begin_src python :results silent
  from mlutils import *
#+end_src

#+begin_src python
  df = read_data('../data/cars.csv')
  clean_alphanum_data(df)
  df.head()
#+end_src

#+RESULTS:
:   make       model  year             engine_fuel_type  engine_hp  engine_cylinders transmission_type  ...                        market_category  vehicle_size vehicle_style highway_mpg city_mpg  popularity   msrp
: 0  bmw  1_series_m  2011  premium_unleaded_(required)      335.0               6.0            manual  ...  factory_tuner,luxury,high-performance       compact         coupe          26       19        3916  46135
: 1  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                     luxury,performance       compact   convertible          28       19        3916  40650
: 2  bmw    1_series  2011  premium_unleaded_(required)      300.0               6.0            manual  ...                luxury,high-performance       compact         coupe          28       20        3916  36350
: 3  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                     luxury,performance       compact         coupe          28       18        3916  29450
: 4  bmw    1_series  2011  premium_unleaded_(required)      230.0               6.0            manual  ...                                 luxury       compact   convertible          28       18        3916  34500
:
: [5 rows x 16 columns]

Split the data set into a train, validation and test set:

#+begin_src python :results silent
  df_train, df_val, df_test = split_data_frame(df, split=0.2, seed=2)

  y_train = np.log1p(df_train.msrp.values)
  y_val = np.log1p(df_val.msrp.values)
  y_test = np.log1p(df_test.msrp.values)
#+end_src

Remove the target value ("msrp" or "manufacturer's suggested retail price") from
the data set:

#+begin_src python :results silent
  del df_train['msrp']
  del df_val['msrp']
  del df_test['msrp']
#+end_src

Prepare the data. To confirm the results in the book (and make sure my code is
working), I'll first use the same parameters:

#+begin_src python :results silent
  # Prepare the training data.
  base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']
  fns = [[encode_age, 'year', 2017],
         [binary_encodes, ["number_of_doors",
                           "make",
                           "engine_fuel_type",
                           "transmission_type",
                           "driven_wheels",
                           "market_category",
                           "vehicle_size",
                           "vehicle_style"],
          5]]
  X_train = prepare_X(df_train, base, fns)

#+end_src

Now train the model:

#+begin_src python :results silent
  w_0, w = linear_regression(X_train, y_train, 0.01)
#+end_src

If we apply the model to the training data, we *should* get the original prices
again. In reality, we don't.

#+begin_src python :results silent
  from matplotlib import pyplot as plt
  import seaborn as sns
#+end_src

#+begin_src python :results file figure/figure2-06.png
  y_pred = w_0 + X_train.dot(w)
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-06.png')
  'figures/figure2-06.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-06.png]]

We can compute the RMSE for the model:

#+begin_src python
  rmse(y_train, y_pred)
#+end_src

#+RESULTS:
: 0.46020995201980425

We should of course compute the RMSE on the validation set:

#+begin_src python
  X_val = prepare_X(df_val, base, fns)
  y_pred = w_0 + X_val.dot(w)
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 0.476510145790575

**** Using more values

Let's follow the suggestion in exercise 2.5.1 and include more values in the
binary encoded features:

#+begin_src python :results file figures/figure2-07.png
  base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']
  fns = [[encode_age, 'year', 2017],
         [binary_encodes, ["number_of_doors",
                           "make",
                           "engine_fuel_type",
                           "transmission_type",
                           "driven_wheels",
                           "market_category",
                           "vehicle_size",
                           "vehicle_style"],
          8]]

  X_train = prepare_X(df_train, base, fns)

  w_0, w = linear_regression(X_train, y_train, 0.01)

  y_pred = w_0 + X_train.dot(w)

  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-07.png')
  'figures/figure2-07.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-07.png]]

Evaluating against the validation set:

#+begin_src python
  X_val = prepare_X(df_val, base, fns)
  y_pred = w_0 + X_val.dot(w)
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 0.4850113356908553

The performance seems to have degraded, not improved, although only by a little.

Note that trying to use 10 values for binary encoding fails, because the
validation set then gains an extra feature. The error reported is:

#+begin_example
  ValueError: shapes (2382,61) and (60,) not aligned: 61 (dim 1) != 60 (dim 0)
#+end_example

I assume that in the validation set, one of the features has one value more than
in the training set.

**** Differentiating more                                       :noexport:

Among the binary encoded features, some have only a few values, others have a
larger number. Cutting them all off at the same point may not be a good idea.

These are the available features, their types and their value counts:

#+begin_src python
  features = []
  for column in df.columns:
      feature = [column, df[column].dtype.name, len(df[column].value_counts())]
      print(feature)
      features.append(feature)

  features
#+end_src

#+RESULTS:
| make              | object  |   48 |
| model             | object  |  914 |
| year              | int64   |   28 |
| engine_fuel_type  | object  |   10 |
| engine_hp         | float64 |  356 |
| engine_cylinders  | float64 |    9 |
| transmission_type | object  |    5 |
| driven_wheels     | object  |    4 |
| number_of_doors   | float64 |    3 |
| market_category   | object  |   71 |
| vehicle_size      | object  |    3 |
| vehicle_style     | object  |   16 |
| highway_mpg       | int64   |   59 |
| city_mpg          | int64   |   69 |
| popularity        | int64   |   48 |
| msrp              | int64   | 6049 |

Let's check which ones were used in the model above:

| feature           | type    | n values | used |
|-------------------+---------+----------+------|
| make              | object  |       48 | *    |
| model             | object  |      914 |      |
| year              | int64   |       28 |      |
| engine_fuel_type  | object  |       10 | *    |
| engine_hp         | float64 |      356 | *    |
| engine_cylinders  | float64 |        9 | *    |
| transmission_type | object  |        5 | *    |
| driven_wheels     | object  |        4 | *    |
| number_of_doors   | float64 |        3 | *    |
| market_category   | object  |       71 | *    |
| vehicle_size      | object  |        3 | *    |
| vehicle_style     | object  |       16 | *    |
| highway_mpg       | int64   |       59 | *    |
| city_mpg          | int64   |       69 | *    |
| popularity        | int64   |       48 | *    |
| msrp              | int64   |     6049 |      |

Not sure if any of these could be engineered better. Let's get on with the next
exercise.

*** House prices

First read the data and clean it up.

#+begin_src python
  df = read_data('../data/housing.csv')
  clean_alphanum_data(df)
  df.head()
#+end_src

#+RESULTS:
:       crim    zn  indus  chas    nox     rm   age     dis  rad    tax  ptratio       b  lstat  medv
: 0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0     15.3  396.90   4.98  24.0
: 1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0     17.8  396.90   9.14  21.6
: 2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0     17.8  392.83   4.03  34.7
: 3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0     18.7  394.63   2.94  33.4
: 4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0     18.7  396.90   5.33  36.2

The header uses abbreviations:

| CRIM    | per capita crime rate by town                                         |
| ZN      | proportion of residential land zoned for lots over 25,000 sq.ft.      |
| INDUS   | proportion of non-retail business acres per town                      |
| CHAS    | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |
| NOX     | nitric oxides concentration (parts per 10 million)                    |
| RM      | average number of rooms per dwelling                                  |
| AGE     | proportion of owner-occupied units built prior to 1940                |
| DIS     | weighted distances to five Boston employment centres                  |
| RAD     | index of accessibility to radial highways                             |
| TAX     | full-value property-tax rate per $10,000                              |
| PTRATIO | pupil-teacher ratio by town                                           |
| B       | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town        |
| LSTAT   | % lower status of the population                                      |
| MEDV    | Median value of owner-occupied homes in $1000’s                       |

#+begin_src python
  df.dtypes
#+end_src

#+RESULTS:
#+begin_example
crim       float64
zn         float64
indus      float64
chas         int64
nox        float64
rm         float64
age        float64
dis        float64
rad          int64
tax        float64
ptratio    float64
b          float64
lstat      float64
medv       float64
dtype: object
#+end_example

The target value is the MEDV. All values except CHAS are numeric, CHAS is
already binary, as it only contains the values 0 and 1. No preprocessing seems
to be necessary.

There are no missing values in the data set:

#+begin_src python
  df.isnull().sum()
#+end_src

#+RESULTS:
#+begin_example
crim       0
zn         0
indus      0
chas       0
nox        0
rm         0
age        0
dis        0
rad        0
tax        0
ptratio    0
b          0
lstat      0
medv       0
dtype: int64
#+end_example

Let's first plot the data:

#+begin_src python :results silent
  from matplotlib import pyplot as plt
  import seaborn as sns
#+end_src

#+begin_src python :results file figures/figure2-09.png
  plt.clf()
  sns.histplot(df.medv, kde=True)
  plt.savefig('figures/figure2-09.png')

  'figures/figure2-09.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-09.png]]

Applying logarithm transformation:

#+begin_src python :results file figures/figure2-10.png
  plt.clf()
  log_price = np.log1p(df.medv)
  sns.histplot(log_price, kde=True)
  plt.savefig('figures/figure2-10.png')

  'figures/figure2-10.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-10.png]]

This doesn't seem to change much. Let's try both ways and see if one comes out
better.

**** No log transformation

Since the data set isn't that big (only 506 items) and we're only playing
around, let's forego creating a test set.

#+begin_src python :results silent
  df_train, df_val, _ = split_data_frame(df, split=0.2, seed=2, test=False)

  y_train = df_train.medv.values
  y_val = df_val.medv.values

  del df_train['medv']
  del df_val['medv']
#+end_src

Prepare the training data:

#+begin_src python :results silent
  base = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']
  X_train = prepare_X(df_train, base)
#+end_src

Train the network:

#+begin_src python :results silent
  w_0, w = linear_regression(X_train, y_train, 0.01)
#+end_src

Plot the results:

#+begin_src python :results file figure/figure2-11.png
  y_pred = w_0 + X_train.dot(w)
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-11.png')
  'figures/figure2-11.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-11.png]]

Not that bad, but too high in the middle-high segment. Let's do validation:

#+begin_src python
  X_val = prepare_X(df_val, base)
  y_pred = w_0 + X_val.dot(w)
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 3.6761997561821307

Ugh.

**** With log transformation

Now let's apply logarithm transformation;

#+begin_src python :results silent
  df_train, df_val, _ = split_data_frame(df, split=0.2, seed=2, test=False)

  y_train = np.log1p(df_train.medv.values)
  y_val = np.log1p(df_val.medv.values)

  del df_train['medv']
  del df_val['medv']
#+end_src

Prepare the training data:

#+begin_src python :results silent
  base = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']
  X_train = prepare_X(df_train, base)
#+end_src

Train the network:

#+begin_src python :results silent
  w_0, w = linear_regression(X_train, y_train, 0.01)
#+end_src

Plot the results:

#+begin_src python :results file figure/figure2-12.png
  y_pred = w_0 + X_train.dot(w)
  plt.clf()
  sns.histplot(y_pred, label='pred')
  sns.histplot(y_train, label='y', color='red')
  plt.legend()
  plt.savefig('figures/figure2-12.png')
  'figures/figure2-12.png'
#+end_src

#+RESULTS:
[[file:figures/figure2-12.png]]

At first sight, not much better than without log transformation. But let's see:

#+begin_src python
  X_val = prepare_X(df_val, base)
  y_pred = w_0 + X_val.dot(w)
  rmse(y_val, y_pred)
#+end_src

#+RESULTS:
: 0.164499584999961

That's a much better result, it seems.


* Chapter 3: Machine Learning for Classification

** Notes to chapter 3                                             :noexport:
:PROPERTIES:
:header-args:python+: :session ch3-notes
:END:

*** Imports

#+begin_src python :results silent
  from mlutils import *

  import seaborn as sns
  from matplotlib import pyplot as plt
#+end_src

*** Initial look at the data

#+begin_src python :results silent
  df = pd.read_csv('../data/telco-customer-churn.csv')
#+end_src

#+begin_src python
  df.head()
#+end_src

#+RESULTS:
:    customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService     MultipleLines  ... StreamingTV StreamingMovies        Contract PaperlessBilling              PaymentMethod MonthlyCharges TotalCharges Churn
: 0  7590-VHVEG  Female              0     Yes         No       1           No  No phone service  ...          No              No  Month-to-month              Yes           Electronic check          29.85        29.85    No
: 1  5575-GNVDE    Male              0      No         No      34          Yes                No  ...          No              No        One year               No               Mailed check          56.95       1889.5    No
: 2  3668-QPYBK    Male              0      No         No       2          Yes                No  ...          No              No  Month-to-month              Yes               Mailed check          53.85       108.15   Yes
: 3  7795-CFOCW    Male              0      No         No      45           No  No phone service  ...          No              No        One year               No  Bank transfer (automatic)          42.30      1840.75    No
: 4  9237-HQITU  Female              0      No         No       2          Yes                No  ...          No              No  Month-to-month              Yes           Electronic check          70.70       151.65   Yes
:
: [5 rows x 21 columns]

Transposing the table gives a better view:

#+begin_src python
  df.head().T
#+end_src

#+RESULTS:
#+begin_example
                                 0             1               2                          3                 4
customerID              7590-VHVEG    5575-GNVDE      3668-QPYBK                 7795-CFOCW        9237-HQITU
gender                      Female          Male            Male                       Male            Female
SeniorCitizen                    0             0               0                          0                 0
Partner                        Yes            No              No                         No                No
Dependents                      No            No              No                         No                No
tenure                           1            34               2                         45                 2
PhoneService                    No           Yes             Yes                         No               Yes
MultipleLines     No phone service            No              No           No phone service                No
InternetService                DSL           DSL             DSL                        DSL       Fiber optic
OnlineSecurity                  No           Yes             Yes                        Yes                No
OnlineBackup                   Yes            No             Yes                         No                No
DeviceProtection                No           Yes              No                        Yes                No
TechSupport                     No            No              No                        Yes                No
StreamingTV                     No            No              No                         No                No
StreamingMovies                 No            No              No                         No                No
Contract            Month-to-month      One year  Month-to-month                   One year    Month-to-month
PaperlessBilling               Yes            No             Yes                         No               Yes
PaymentMethod     Electronic check  Mailed check    Mailed check  Bank transfer (automatic)  Electronic check
MonthlyCharges               29.85         56.95           53.85                       42.3              70.7
TotalCharges                 29.85        1889.5          108.15                    1840.75            151.65
Churn                           No            No             Yes                         No               Yes
#+end_example

#+begin_src python
  df.dtypes
#+end_src

#+RESULTS:
#+begin_example
customerID           object
gender               object
SeniorCitizen         int64
Partner              object
Dependents           object
tenure                int64
PhoneService         object
MultipleLines        object
InternetService      object
OnlineSecurity       object
OnlineBackup         object
DeviceProtection     object
TechSupport          object
StreamingTV          object
StreamingMovies      object
Contract             object
PaperlessBilling     object
PaymentMethod        object
MonthlyCharges      float64
TotalCharges         object
Churn                object
dtype: object
#+end_example

Note that 'TotalCharges' is of type 'object', even though we'd expect it to be a
number. There are probably non-numeric characters in this field:

#+begin_src python
  total_charges = pd.to_numeric(df['TotalCharges'], errors='coerce')  # Convert to number, do not fail on errors.
  df[total_charges.isnull()][['customerID', 'TotalCharges']]          # List those where TotalCharges is empty.
#+end_src

#+RESULTS:
#+begin_example
      customerID TotalCharges
488   4472-LVYGI
753   3115-CZMZD
936   5709-LVOEQ
1082  4367-NUYAO
1340  1371-DWPAZ
3331  7644-OMVMY
3826  3213-VVOLG
4380  2520-SGTTA
5218  2923-ARZLG
6670  4075-WKNIU
6754  2775-SEFEE
#+end_example

This shows that there are some entries where "TotalCharges" contains whitespace.

#+begin_remark
This is interesting. =df[total_charges.isnull()]= produces a dataframe of those
elements in =df= where =total_charges.isnull()= is =True=. =total_charges=
itself obviously has to be a dataframe of the same number of rows as =df=. Note:
=total_charges.isnull()= produces a dataframe of one column.
#+end_remark

We simply set the missing values to 0.

#+begin_remark
Presumably, that is exactly what's intended here, since 'tenure' is 0 in all of them:
#+end_remark

#+begin_src python
  df[df.tenure == 0]
#+end_src

#+RESULTS:
#+begin_example
      customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService     MultipleLines  ...          StreamingTV      StreamingMovies  Contract PaperlessBilling              PaymentMethod MonthlyCharges TotalCharges Churn
488   4472-LVYGI  Female              0     Yes        Yes       0           No  No phone service  ...                  Yes                   No  Two year              Yes  Bank transfer (automatic)          52.55                 No
753   3115-CZMZD    Male              0      No        Yes       0          Yes                No  ...  No internet service  No internet service  Two year               No               Mailed check          20.25                 No
936   5709-LVOEQ  Female              0     Yes        Yes       0          Yes                No  ...                  Yes                  Yes  Two year               No               Mailed check          80.85                 No
1082  4367-NUYAO    Male              0     Yes        Yes       0          Yes               Yes  ...  No internet service  No internet service  Two year               No               Mailed check          25.75                 No
1340  1371-DWPAZ  Female              0     Yes        Yes       0           No  No phone service  ...                  Yes                   No  Two year               No    Credit card (automatic)          56.05                 No
3331  7644-OMVMY    Male              0     Yes        Yes       0          Yes                No  ...  No internet service  No internet service  Two year               No               Mailed check          19.85                 No
3826  3213-VVOLG    Male              0     Yes        Yes       0          Yes               Yes  ...  No internet service  No internet service  Two year               No               Mailed check          25.35                 No
4380  2520-SGTTA  Female              0     Yes        Yes       0          Yes                No  ...  No internet service  No internet service  Two year               No               Mailed check          20.00                 No
5218  2923-ARZLG    Male              0     Yes        Yes       0          Yes                No  ...  No internet service  No internet service  One year              Yes               Mailed check          19.70                 No
6670  4075-WKNIU  Female              0     Yes        Yes       0          Yes               Yes  ...                  Yes                   No  Two year               No               Mailed check          73.35                 No
6754  2775-SEFEE    Male              0      No        Yes       0          Yes               Yes  ...                   No                   No  Two year              Yes  Bank transfer (automatic)          61.90                 No

[11 rows x 21 columns]
#+end_example

#+begin_remark
Those are the same customers as above.
#+end_remark

#+begin_src python :results silent
  df.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')
  df.TotalCharges = df.TotalCharges.fillna(0)
#+end_src

We clean up the column names and alphanumeric data as before:

#+begin_src python :results silent
  clean_alphanum_data(df)
#+end_src

The 'churn' column has 'yes/no' as its value, but this should be 1/0:

#+begin_src python :results silent
  df.churn = (df.churn == 'yes').astype(int)
#+end_src

Instead of our own function, we use Scikit-Learn for splitting the data:

#+begin_src python :results silent
  from sklearn.model_selection import train_test_split
#+end_src

=train_test_split= only splits a dataframe into two dataframes, so in order to
get a train, test and validation set, we need to call it twice:

#+begin_src python :results silent
  df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)
  df_train, df_val = train_test_split(df_train_full, test_size=0.2, random_state=11)

  y_train = df_train.churn.values
  y_val = df_val.churn.values

  del df_train['churn']
  del df_val['churn']
#+end_src

*** Exploratory data analysis

There are no missing values in the dataset:

#+begin_src python
  df_train_full.isnull().sum()
#+end_src

#+RESULTS:
#+begin_example
customerid          0
gender              0
seniorcitizen       0
partner             0
dependents          0
tenure              0
phoneservice        0
multiplelines       0
internetservice     0
onlinesecurity      0
onlinebackup        0
deviceprotection    0
techsupport         0
streamingtv         0
streamingmovies     0
contract            0
paperlessbilling    0
paymentmethod       0
monthlycharges      0
totalcharges        0
churn               0
dtype: int64
#+end_example

We should also check the distribution of values:

#+begin_src python
  df_train_full.churn.value_counts()
#+end_src

#+RESULTS:
: 0    4113
: 1    1521
: Name: churn, dtype: int64

We can calculate the churn rate:

\[
\mathtt{churn\_rate} = \frac{\mathtt{number\_churned}}{\mathtt{number\_not\_churned}}
\]

Or by using the =mean()= method, given that 'churn' has the value 1 for
customers that churned and 0 for customers that did not churn:

#+begin_src python
  global_mean = df_train_full.churn.mean()
  round(global_mean, 3)
#+end_src

#+RESULTS:
: 0.27

The dataset is *imbalanced*, because the proportion of churned vs. non-churned
is not 50/50.

We separate the categorical and numerical variables:

 #+begin_src python :results silent
   categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',
                  'phoneservice', 'multiplelines', 'internetservice',
                  'onlinesecurity', 'onlinebackup', 'deviceprotection',
                  'techsupport', 'streamingtv', 'streamingmovies',
                  'contract', 'paperlessbilling', 'paymentmethod']
   numerical = ['tenure', 'monthlycharges', 'totalcharges']
 #+end_src

The categorical features should only have a few values:

#+begin_src python
  df_train_full[categorical].nunique()
#+end_src

#+RESULTS:
#+begin_example
gender              2
seniorcitizen       2
partner             2
dependents          2
phoneservice        2
multiplelines       3
internetservice     3
onlinesecurity      3
onlinebackup        3
deviceprotection    3
techsupport         3
streamingtv         3
streamingmovies     3
contract            3
paperlessbilling    2
paymentmethod       4
dtype: int64
#+end_example

The data is apparently clean, no preprocessing needs to be done.

*** Feature importance

*Feature importance analysis* is the process of finding out which features have
a greater influence on the target value. Numerical and categorical features
require different methods of analysis.

**** Churn rate

For categorical variables, we can check the churn rate for each variable. If it
is similar to the global churn rate, the variable is not important for the churn
rate. If the difference is larger, there is something about this variable that
sets it apart from other variables.

Take the 'gender' feature as an example:

#+begin_src python :results output
  female_mean = df_train_full[df_train_full.gender == 'female'].churn.mean()
  male_mean = df_train_full[df_train_full.gender == 'male'].churn.mean()

  print('gender == female:', round(female_mean, 3))
  print('gender == male:  ', round(male_mean, 3))
#+end_src

#+RESULTS:
: gender == female: 0.277
: gender == male:   0.263

The values for 'partner' are different:

#+begin_src python :results output
  partner_yes = df_train_full[df_train_full.partner == 'yes'].churn.mean()
  partner_no = df_train_full[df_train_full.partner == 'no'].churn.mean()

  print('partner == yes:', round(partner_yes, 3))
  print('partner == no:  ', round(partner_no, 3))
#+end_src

#+RESULTS:
: partner == yes: 0.205
: partner == no:   0.33

In this case, the difference with the global churn rate is larger, so the
variable 'partner' is useful for predicting churn.

**** Risk ratio

The *risk ratio* is the ratio of the group churn rate and the global churn rate:

\[
\mathtt{risk} = \frac{\mathtt{group\_churn}}{\mathtt{global\_churn}}
\]

For gender, the risk ratio is close to 1:

#+begin_src python :results output
  print('female risk: ', female_mean / global_mean)
  print('male risk  : ', male_mean / global_mean)
#+end_src

#+RESULTS:
: female risk:  1.0253955354648652
: male risk  :  0.9749802969838747

The risk ratio is a number between 0 and ∞. If it is close to 1, the risk of
churning in the group is similar to the risk of churning in the population as a
whole. If the risk ratio is lower than 1, the group's risk is lower than the
risk of the total population, and if the value is higher than 1, the group's
risk is higher than the risk of the total population.

#+begin_src python :results output
  print('with partner risk   : ', partner_yes / global_mean)
  print('without partner risk: ', partner_no / global_mean)
#+end_src

#+RESULTS:
: with partner risk   :  0.7594724924338315
: without partner risk:  1.2216593879412643

To do this calculation for all categorical variables, we need some code that
calculates the churn rate for all values of a variable:

#+begin_src python
  df_group = df_train_full.groupby(by='gender').churn.agg(['mean'])
  df_group['diff'] = df_group['mean'] - global_mean
  df_group['risk'] = df_group['mean'] / global_mean
  df_group
#+end_src

#+RESULTS:
:             mean      diff      risk
: gender
: female  0.276824  0.006856  1.025396
: male    0.263214 -0.006755  0.974980

#+begin_remark
This is some interesting code. The =groupby= method creates a =GroupBy= object,
in which the different values of the relevant variable are grouped, allowing
aggregate functions to be applied to them. Aggregate functions are functions
that apply over all of the elements in the group, such as =mean=, =min=, =max=,
etc. The function to apply can be supplied as a string, as done here.
#+end_remark

We can do this for all categorical features:

#+begin_src python :results output
  for col in categorical:
      df_group = df_train_full.groupby(by=col).churn.agg(['mean'])
      df_group['diff'] = df_group['mean'] - global_mean
      df_group['risk'] = df_group['mean'] / global_mean
      print(df_group)
      print('\n========================================\n')
#+end_src

#+RESULTS:
#+begin_example
            mean      diff      risk
gender
female  0.276824  0.006856  1.025396
male    0.263214 -0.006755  0.974980

========================================

                   mean      diff      risk
seniorcitizen
0              0.242270 -0.027698  0.897403
1              0.413377  0.143409  1.531208

========================================

             mean      diff      risk
partner
no       0.329809  0.059841  1.221659
yes      0.205033 -0.064935  0.759472

========================================

                mean      diff      risk
dependents
no          0.313760  0.043792  1.162212
yes         0.165666 -0.104302  0.613651

========================================

                  mean      diff      risk
phoneservice
no            0.241316 -0.028652  0.893870
yes           0.273049  0.003081  1.011412

========================================

                      mean      diff      risk
multiplelines
no                0.257407 -0.012561  0.953474
no_phone_service  0.241316 -0.028652  0.893870
yes               0.290742  0.020773  1.076948

========================================

                     mean      diff      risk
internetservice
dsl              0.192347 -0.077621  0.712482
fiber_optic      0.425171  0.155203  1.574895
no               0.077805 -0.192163  0.288201

========================================

                         mean      diff      risk
onlinesecurity
no                   0.420921  0.150953  1.559152
no_internet_service  0.077805 -0.192163  0.288201
yes                  0.153226 -0.116742  0.567570

========================================

                         mean      diff      risk
onlinebackup
no                   0.404323  0.134355  1.497672
no_internet_service  0.077805 -0.192163  0.288201
yes                  0.217232 -0.052736  0.804660

========================================

                         mean      diff      risk
deviceprotection
no                   0.395875  0.125907  1.466379
no_internet_service  0.077805 -0.192163  0.288201
yes                  0.230412 -0.039556  0.853480

========================================

                         mean      diff      risk
techsupport
no                   0.418914  0.148946  1.551717
no_internet_service  0.077805 -0.192163  0.288201
yes                  0.159926 -0.110042  0.592390

========================================

                         mean      diff      risk
streamingtv
no                   0.342832  0.072864  1.269897
no_internet_service  0.077805 -0.192163  0.288201
yes                  0.302723  0.032755  1.121328

========================================

                         mean      diff      risk
streamingmovies
no                   0.338906  0.068938  1.255358
no_internet_service  0.077805 -0.192163  0.288201
yes                  0.307273  0.037305  1.138182

========================================

                    mean      diff      risk
contract
month-to-month  0.431701  0.161733  1.599082
one_year        0.120573 -0.149395  0.446621
two_year        0.028274 -0.241694  0.104730

========================================

                      mean      diff      risk
paperlessbilling
no                0.172071 -0.097897  0.637375
yes               0.338151  0.068183  1.252560

========================================

                               mean      diff      risk
paymentmethod
bank_transfer_(automatic)  0.168171 -0.101797  0.622928
credit_card_(automatic)    0.164339 -0.105630  0.608733
electronic_check           0.455890  0.185922  1.688682
mailed_check               0.193870 -0.076098  0.718121

========================================
#+end_example

These tables tell us a lot. We see that e.g., 'gender' is not relevant for the
churn rate. Senior citizens tend to churn more than non-seniors (or die more?),
etc.

Looking at 'techsupport' and 'contract', we see that clients without tech
support churn more than those with tech support and that people with monthly
contracts churn more than those with one- or two-year contracts.

It is not really possible to see from this data which variable is more useful /
important for predicting churn, however. To determine this, we can measure the
degree of dependence between a categorical variable and the target value using
various metrics. For categorical variables, *mutual information* is one such
metric. Higher values of mutual information mean a higher degree of dependence.

In Scikit-Learn, mutual information can be calculated with
=mutual_info_score=:

#+begin_src python
  from sklearn.metrics import mutual_info_score

  def calculate_mi(series):
      return mutual_info_score(series, df_train_full.churn)

  df_mi = df_train_full[categorical].apply(calculate_mi)
  df_mi = df_mi.sort_values(ascending=False).to_frame(name='MI')
  df_mi
#+end_src

#+RESULTS:
#+begin_example
                        MI
contract          0.098320
onlinesecurity    0.063085
techsupport       0.061032
internetservice   0.055868
onlinebackup      0.046923
deviceprotection  0.043453
paymentmethod     0.043210
streamingtv       0.031853
streamingmovies   0.031581
paperlessbilling  0.017589
dependents        0.012346
partner           0.009968
seniorcitizen     0.009410
multiplelines     0.000857
phoneservice      0.000229
gender            0.000117
#+end_example

'contract', 'onlinesecurity' and 'techsupport' are the most important features
for predicting churn. 'gender' is very uninformative.

Note that *mutual information* only works if both variables are categorical.
Since the target value here is either 0 or 1, it is categorical. *Mutual
information* doesn't work if one of the variables is numerical, however.

To estimate the importance of the three numerical variables that we have, we can
pretend the target value is numerical as well. One way to do this is to use the
*correlation coefficient*. This value indicates what one variable does if the
other changes:

- A positive correlation means that if one variable goes up, the other does, as
  well. For a binary target, this means more ones than zeros.
- A zero correlation means the variables are independent.
- A negative correlation means that if one variable goes up, the other goes down
  (or more zeros appear).

In Pandas:

#+begin_src python
  df_train_full[numerical].corrwith(df_train_full.churn)
#+end_src

#+RESULTS:
: tenure           -0.351885
: monthlycharges    0.196805
: totalcharges     -0.196353
: dtype: float64

*** Feature engineering

In Scikit-Learn, one-hot encoding of categorical features can be achieved in
various ways. We will use =DictVectorizer=. This takes a list of dictionaries
and creates a Numpy array of vectorized features. Numerical features are not
touched by this method.

First, create a dictionary from the training set:

#+begin_src python :results silent
  train_dict = df_train[categorical + numerical].to_dict('rows')
#+end_src

#+begin_src python :results output
  import pprint

  pprint.pprint(train_dict[0])
#+end_src

#+RESULTS:
#+begin_example
{'contract': 'month-to-month',
 'dependents': 'no',
 'deviceprotection': 'no',
 'gender': 'male',
 'internetservice': 'dsl',
 'monthlycharges': 44.65,
 'multiplelines': 'no_phone_service',
 'onlinebackup': 'yes',
 'onlinesecurity': 'no',
 'paperlessbilling': 'yes',
 'partner': 'no',
 'paymentmethod': 'mailed_check',
 'phoneservice': 'no',
 'seniorcitizen': 0,
 'streamingmovies': 'yes',
 'streamingtv': 'no',
 'techsupport': 'yes',
 'tenure': 1,
 'totalcharges': 44.65}
#+end_example

The =DictVectorizer= needs to be fit to the list of dictionaries:

#+begin_src python :results silent
  from sklearn.feature_extraction import DictVectorizer

  dv = DictVectorizer(sparse=False)
  dv.fit(train_dict)
#+end_src

The =fit= method checks the list of dictionaries and figures out how many
possible values the categorical features have. Numerical features are left
alone. We can now transform the list of dictionaries:

#+begin_src python :results silent
  X_train = dv.transform(train_dict)
#+end_src

Check the result:

#+begin_src python
  X_train[0]
#+end_src

#+RESULTS:
| 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 44.65 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 44.65 |

*** Machine learning for classification

**** Logistic regression

Logistic regression is a linear model, but despite its name it is not a
regression model, it's a classification model. In particular, it's a binary
classification model, the target value is 0 (the effect does not occur) or 1
(the effect occurs). The actual output is the probability that the effect
occurs.

A probability must be a number between 0 and 1. To ensure that the model's
output lies between 0 and 1, we use the sigmoid function. The formula for the
logistic regression function is:

\[
g(x_{i}) = S(w_{0} + x^{T}_{}_{i} w)
\]

The dot product \(x^{T}_{i} w\) can be can be unwrapped as a sum:

\[
g(x_{i}) = S(w_{0} + \sum^{n}_{j=1}^{} x_{ij}w_{j})
\]

In Python, logistic regression can be implemented similar to linear regression:

#+begin_src python :results silent
  import math

  def logistic_regression(xi):
      score = bias
      for j in range(n):
          score = score + xi[j] * w[j]
      prob = sigmoid(score)
      return prob

  def sigmoid(score):
      return 1 / (1 + math.exp(-score))
#+end_src

#+begin_remark
The code for =logistic_regression= isn't quite right. =bias= and =w= aren't
defined. The book repeats the definition of =linear_regression=, but it doesn't
conform to the definition of =linear_regression= in chapter 2.
#+end_remark

**** Training logistic regression

We need to import the module:

#+begin_src python :results silent
  from sklearn.linear_model import LogisticRegression
#+end_src

Then create a model instance and train it:

#+begin_src python :results silent
  model = LogisticRegression(solver='liblinear', random_state=1)
  model.fit(X_train, y_train)
#+end_src

Let us test the model on the validation set. First, we need to apply the same
feature engineering to the validation set:

#+begin_src python :results silent
  val_dict = df_val[categorical + numerical].to_dict('rows')
  X_val = dv.transform(val_dict)
#+end_src

We can now create predictions for our validation set. The method to use is
=predict_proba=:

#+begin_src python :results silent
  y_pred = model.predict_proba(X_val)
#+end_src

Let us take a look:

#+begin_src python :results output
  print(y_pred)
#+end_src

#+RESULTS:
: [[0.77380038 0.22619962]
:  [0.77323804 0.22676196]
:  [0.72712399 0.27287601]
:  ...
:  [0.9924975  0.0075025 ]
:  [0.93711735 0.06288265]
:  [0.99569786 0.00430214]]

The result is a Numpy array of two columns: the first column contains the
probability that the target is negative (no churn in our case) and the second
contains the probability that the target is positive (churn).

The two columns convey the same information, though: the sum of the two
probabilities should always be 1. We can therefore discard one of the columns:

#+begin_src python :results silent
  y_pred = model.predict_proba(X_val)[:, 1]
#+end_src

These are so-called /soft/ predictions, because they predict the probability. In
the current case, however, we need /hard/ predictions, i.e., =True= or =False=.
We will have to set an arbitrary threshold, say 0.5:

#+begin_src python :results silent
  churn = y_pred >= 0.5
#+end_src

We now have the predictions in a form that we can use to evaluate the model. One
quality measure is *accuracy*, which can be calculated in Numpy as follows:

#+begin_src python
  (y_val == churn).mean()
#+end_src

#+RESULTS:
: 0.7923691215616682

Note that =y_val= contains values 0 and 1, while =churn= contains =True= and
=False=. They can nonetheless be compared, because Numpy casts =True= to 1 and
=False= to 0.

**** Model interpretation

The logistic regression model learns two parameters from the data:

- w_{0}: the bias.
- w = (w_{1}, w_{2}, ..., w_{n}): the weights vector.

In Scikit-Learn, w_{0} is accessible through =model.intercept_[0]=, the weights are
stored in =model.coef_[0]=.

#+begin_src python :results output
  print("bias: ", model.intercept_[0], "\nbaseline probability: ", sigmoid(model.intercept_[0]))
#+end_src

#+RESULTS:
: bias:  -0.12335101039997287
: baseline probability:  0.4692012889104188

The bias is the baseline prediction. If all other features are 0, the bias is
the predicted result. In the case of logistic regression, we would still need to
apply the sigmoid function. Here, the baseline probability is 47%, meaning that
without knowing any other features, we must assume that it is almost as likely
that a customer will churn as he is likely to stay.

The feature names are available from the =DictVectorizer= object through the
=get_feature_names= method:

#+begin_src python :results output
  pprint.pprint(dict(zip(dv.get_feature_names(), model.coef_[0].round(3))))
#+end_src

#+RESULTS:
#+begin_example
{'contract=month-to-month': 0.556,
 'contract=one_year': -0.184,
 'contract=two_year': -0.496,
 'dependents=no': -0.009,
 'dependents=yes': -0.114,
 'deviceprotection=no': 0.061,
 'deviceprotection=no_internet_service': -0.106,
 'deviceprotection=yes': -0.078,
 'gender=female': -0.043,
 'gender=male': -0.081,
 'internetservice=dsl': -0.353,
 'internetservice=fiber_optic': 0.336,
 'internetservice=no': -0.106,
 'monthlycharges': 0.001,
 'multiplelines=no': -0.17,
 'multiplelines=no_phone_service': 0.12,
 'multiplelines=yes': -0.074,
 'onlinebackup=no': 0.111,
 'onlinebackup=no_internet_service': -0.106,
 'onlinebackup=yes': -0.128,
 'onlinesecurity=no': 0.261,
 'onlinesecurity=no_internet_service': -0.106,
 'onlinesecurity=yes': -0.278,
 'paperlessbilling=no': -0.22,
 'paperlessbilling=yes': 0.096,
 'partner=no': -0.099,
 'partner=yes': -0.024,
 'paymentmethod=bank_transfer_(automatic)': -0.092,
 'paymentmethod=credit_card_(automatic)': -0.11,
 'paymentmethod=electronic_check': 0.242,
 'paymentmethod=mailed_check': -0.163,
 'phoneservice=no': 0.12,
 'phoneservice=yes': -0.244,
 'seniorcitizen': 0.258,
 'streamingmovies=no': -0.086,
 'streamingmovies=no_internet_service': -0.106,
 'streamingmovies=yes': 0.069,
 'streamingtv=no': -0.093,
 'streamingtv=no_internet_service': -0.106,
 'streamingtv=yes': 0.076,
 'techsupport=no': 0.218,
 'techsupport=no_internet_service': -0.106,
 'techsupport=yes': -0.235,
 'tenure': -0.067,
 'totalcharges': 0.0}
#+end_example

A negative weight decreases the final value and therefore reduces the
probability that the customer will churn. Positive weights raise the
probability.

**** Using the model

Let us apply the model to a customer for scoring. In order to do this, a
customer must be represented in the same way as the training data. First we put
the customer data in a dictionary:

#+begin_src python :results silent
  customer = {
      'customerid':       '8879-zkjof',
      'gender':           'female',
      'seniorcitizen':     0,
      'partner':          'no',
      'dependents':       'no',
      'tenure':            41,
      'phoneservice':     'yes',
      'multiplelines':    'no',
      'internetservice':  'dsl',
      'onlinesecurity':   'yes',
      'onlinebackup':     'no',
      'deviceprotection': 'yes',
      'techsupport':      'yes',
      'streamingtv':      'yes',
      'streamingmovies':  'yes',
      'contract':         'one_year',
      'paperlessbilling': 'yes',
      'paymentmethod':    'bank_transfer_(automatic)',
      'monthlycharges':    79.85,
      'totalcharges':      3320.75
  }
#+end_src

This dict is converted to a matrix using the =DictVectorizer=. Note that the
=DictVectorizer= object expects a list of dictionaries:

#+begin_src python :results silent
  X_test = dv.transform([customer])
#+end_src

This results in a matrix with one row:

#+begin_src python :results verbatim
  X_test
#+end_src

#+RESULTS:
: [[0.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00
:   0.00000e+00 1.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00
:   0.00000e+00 7.98500e+01 1.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00
:   0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00
:   1.00000e+00 1.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00
:   0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00
:   1.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00
:   1.00000e+00 4.10000e+01 3.32075e+03]]

This can then be fed to the model to create a prediction. As with the validation
data, this yields a two-column array (with only one row), but we only need the
value from the second column:

#+begin_src python
  model.predict_proba(X_test)[0, 1]
#+end_src

#+RESULTS:
: 0.053396497111509275

The probability of this customer churning is very low.

Scoring another client:

#+begin_src python :results silent
  customer = {
      'gender':           'female',
      'seniorcitizen':     1,
      'partner':          'no',
      'dependents':       'no',
      'phoneservice':     'yes',
      'multiplelines':    'yes',
      'internetservice':  'fiber_optic',
      'onlinesecurity':   'no',
      'onlinebackup':     'no',
      'deviceprotection': 'no',
      'techsupport':      'no',
      'streamingtv':      'yes',
      'streamingmovies':  'no',
      'contract':         'month-to-month',
      'paperlessbilling': 'yes',
      'paymentmethod':    'electronic_check',
      'tenure':            1,
      'monthlycharges':    85.7,
      'totalcharges':      85.7
  }
#+end_src

#+begin_src python
  X_test = dv.transform([customer])
  model.predict_proba(X_test)[0,1]
#+end_src

#+RESULTS:
: 0.8287013608698214

This customer's probability of churning is high.


** Exercises and code

*** Lead scoring
:PROPERTIES:
:header-args:python+: :session ch3-ex1
:END:

**** Initial look at the data

The data contains the following fields:

| Variables                                        | Description                                                                                                                        |
|--------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------|
| Prospect ID                                      | A unique ID with which the customer is identified.                                                                                 |
| Lead Number                                      | A lead number assigned to each lead procured.                                                                                      |
| Lead Origin                                      | The origin identifier with which the customer was identified to be a lead. Includes API, Landing Page Submission, etc.             |
| Lead Source                                      | The source of the lead. Includes Google, Organic Search, Olark Chat, etc.                                                          |
| Do Not Email                                     | An indicator variable selected by the customer wherein they select whether of not they want to be emailed about the course or not. |
| Do Not Call                                      | An indicator variable selected by the customer wherein they select whether of not they want to be called about the course or not.  |
| Converted                                        | The target variable. Indicates whether a lead has been successfully converted or not.                                              |
| TotalVisits                                      | The total number of visits made by the customer on the website.                                                                    |
| Total Time Spent on Website                      | The total time spent by the customer on the website.                                                                               |
| Page Views Per Visit                             | Average number of pages on the website viewed during the visits.                                                                   |
| Last Activity                                    | Last activity performed by the customer. Includes Email Opened, Olark Chat Conversation, etc.                                      |
| Country                                          | The country of the customer.                                                                                                       |
| Specialization                                   | The industry domain in which the customer worked before.                                                                           |
| How did you hear about X Education               | The source from which the customer heard about X Education.                                                                        |
| What is your current occupation                  | Indicates whether the customer is a student, umemployed or employed.                                                               |
| What matters most to you in choosing this course | An option selected by the customer indicating what is their main motto behind doing this course.                                   |
| Search                                           | Indicating whether the customer had seen the ad in any of the listed items.                                                        |
| Magazine                                         |                                                                                                                                    |
| Newspaper Article                                |                                                                                                                                    |
| X Education Forums                               |                                                                                                                                    |
| Newspaper                                        |                                                                                                                                    |
| Digital Advertisement                            |                                                                                                                                    |
| Through Recommendations                          | Indicates whether the customer came in through recommendations.                                                                    |
| Receive More Updates About Our Courses           | Indicates whether the customer chose to receive more updates about the courses.                                                    |
| Tags                                             | Tags assigned to customers indicating the current status of the lead.                                                              |
| Lead Quality                                     | Indicates the quality of lead based on the data and intuition the the employee who has been assigned to the lead.                  |
| Update me on Supply Chain Content                | Indicates whether the customer wants updates on the Supply Chain Content.                                                          |
| Get updates on DM Content                        | Indicates whether the customer wants updates on the DM Content.                                                                    |
| Lead Profile                                     | A lead level assigned to each customer based on their profile.                                                                     |
| City                                             | The city of the customer.                                                                                                          |
| Asymmetrique Activity Index                      | An index and score assigned to each customer based on their activity and their profile                                             |
| Asymmetrique Profile Index                       |                                                                                                                                    |
| Asymmetrique Activity Score                      |                                                                                                                                    |
| Asymmetrique Profile Score                       |                                                                                                                                    |
| I agree to pay the amount through cheque         | Indicates whether the customer has agreed to pay the amount through cheque or not.                                                 |
| a free copy of Mastering The Interview           | Indicates whether the customer wants a free copy of 'Mastering the Interview' or not.                                              |
| Last Notable Activity                            | The last notable acitivity performed by the student.                                                                               |

Note: The field "Specialization" includes the level 'Select Specialization'
which means the customer has not selected this option while filling the form.

**** Imports

#+begin_src python :results silent
  from mlutils import *

  import numpy as np
  import pandas as pd
#+end_src

**** Read the data:

#+begin_src python :results silent
  df = pd.read_csv('../data/Leads.csv')
#+end_src

#+begin_src python
  df.head(4).T
#+end_src

#+RESULTS:
#+begin_example
                                                                                  0                                     1                                     2                                     3
Prospect ID                                    7927b2df-8bba-4d29-b9a2-b6e0beafe620  2a272436-5132-4136-86fa-dcc88c88f482  8cc8c611-a219-4f35-ad23-fdfd2656bd8a  0cc2df48-7cf4-4e39-9de9-19797f9b38cc
Lead Number                                                                  660737                                660728                                660727                                660719
Lead Origin                                                                     API                                   API               Landing Page Submission               Landing Page Submission
Lead Source                                                              Olark Chat                        Organic Search                        Direct Traffic                        Direct Traffic
Do Not Email                                                                     No                                    No                                    No                                    No
Do Not Call                                                                      No                                    No                                    No                                    No
Converted                                                                         0                                     0                                     1                                     0
TotalVisits                                                                       0                                     5                                     2                                     1
Total Time Spent on Website                                                       0                                   674                                  1532                                   305
Page Views Per Visit                                                              0                                   2.5                                     2                                     1
Last Activity                                               Page Visited on Website                          Email Opened                          Email Opened                           Unreachable
Country                                                                         NaN                                 India                                 India                                 India
Specialization                                                               Select                                Select               Business Administration                 Media and Advertising
How did you hear about X Education                                           Select                                Select                                Select                         Word Of Mouth
What is your current occupation                                          Unemployed                            Unemployed                               Student                            Unemployed
What matters most to you in choosing a course               Better Career Prospects               Better Career Prospects               Better Career Prospects               Better Career Prospects
Search                                                                           No                                    No                                    No                                    No
Magazine                                                                         No                                    No                                    No                                    No
Newspaper Article                                                                No                                    No                                    No                                    No
X Education Forums                                                               No                                    No                                    No                                    No
Newspaper                                                                        No                                    No                                    No                                    No
Digital Advertisement                                                            No                                    No                                    No                                    No
Through Recommendations                                                          No                                    No                                    No                                    No
Receive More Updates About Our Courses                                           No                                    No                                    No                                    No
Tags                                                    Interested in other courses                               Ringing   Will revert after reading the email                               Ringing
Lead Quality                                                       Low in Relevance                                   NaN                              Might be                              Not Sure
Update me on Supply Chain Content                                                No                                    No                                    No                                    No
Get updates on DM Content                                                        No                                    No                                    No                                    No
Lead Profile                                                                 Select                                Select                        Potential Lead                                Select
City                                                                         Select                                Select                                Mumbai                                Mumbai
Asymmetrique Activity Index                                               02.Medium                             02.Medium                             02.Medium                             02.Medium
Asymmetrique Profile Index                                                02.Medium                             02.Medium                               01.High                               01.High
Asymmetrique Activity Score                                                      15                                    15                                    14                                    13
Asymmetrique Profile Score                                                       15                                    15                                    20                                    17
I agree to pay the amount through cheque                                         No                                    No                                    No                                    No
A free copy of Mastering The Interview                                           No                                    No                                   Yes                                    No
Last Notable Activity                                                      Modified                          Email Opened                          Email Opened                              Modified
#+end_example

**** Normalise alphanumeric data:

#+begin_src python :results silent
  clean_alphanum_data(df)
#+end_src

**** Variable types

The types of the variables:

#+begin_src python
  df.dtypes
#+end_src

#+RESULTS:
#+begin_example
prospect_id                                       object
lead_number                                        int64
lead_origin                                       object
lead_source                                       object
do_not_email                                      object
do_not_call                                       object
converted                                          int64
totalvisits                                      float64
total_time_spent_on_website                        int64
page_views_per_visit                             float64
last_activity                                     object
country                                           object
specialization                                    object
how_did_you_hear_about_x_education                object
what_is_your_current_occupation                   object
what_matters_most_to_you_in_choosing_a_course     object
search                                            object
magazine                                          object
newspaper_article                                 object
x_education_forums                                object
newspaper                                         object
digital_advertisement                             object
through_recommendations                           object
receive_more_updates_about_our_courses            object
tags                                              object
lead_quality                                      object
update_me_on_supply_chain_content                 object
get_updates_on_dm_content                         object
lead_profile                                      object
city                                              object
asymmetrique_activity_index                       object
asymmetrique_profile_index                        object
asymmetrique_activity_score                      float64
asymmetrique_profile_score                       float64
i_agree_to_pay_the_amount_through_cheque          object
a_free_copy_of_mastering_the_interview            object
last_notable_activity                             object
dtype: object
#+end_example

**** Check the target value:

The target variable is 'Converted', which is numerical. Let's see if it's
binary:

#+begin_src python
  df.converted.value_counts()
#+end_src

#+RESULTS:
: 0    5679
: 1    3561
: Name: converted, dtype: int64

The dataset is imbalanced:

#+begin_src python
  global_mean = df.converted.mean()
  round(global_mean, 3)
#+end_src

#+RESULTS:
: 0.385

#+begin_src python :results silent
  from sklearn.model_selection import train_test_split

  df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)
  df_train, df_val = train_test_split(df_train_full, test_size=0.2, random_state=11)

  y_train = df_train.converted.values
  y_val = df_val.converted.values

  del df_train['converted']
  del df_val['converted']
#+end_src

*** Default prediction
:PROPERTIES:
:header-args:python+: :session ch3-ex2
:END:


* COMMENT Local Variables
:PROPERTIES:
:VISIBILITY: folded
:END:
# Local Variables:
# eval: (guess-language-mode -1)
# ispell-local-dictionary: "english"
# eval: (visual-fill-column-mode -1)
# eval: (auto-fill-mode 1))
# eval: (hl-line-mode 1)
# eval: (auto-revert-mode 1)
# eval: (mixed-pitch-mode -1)
# End:
